{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../log/bwdataset_2022-05-17.log\n",
      "logfilepath:../log/qnadataset_2022-05-17.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-ft_2022-05-17.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# kogpt2 를 이용해 훈련한 Q&A 훈련 예시\n",
    "#\n",
    "# => train loss, val loss 만 구함\n",
    "#\n",
    "# => 여기서는 훈련할때 <question>, <answer> 2개의 구분자 토큰을 지정하였음. \n",
    "# [훈련 dataset]\n",
    "# => input_ids = '지문<qeustion>질문<answer>답변</s>'   \n",
    "# => labels = input_ids와 동일\n",
    "#\n",
    "# [Q&A 훈련 과정]\n",
    "# \n",
    "# 1. gpt-2 모델 선언(GPT2LMHeadModel), tokenizer 선언(PreTrainedTokenizerFast)\n",
    "# 2. '지문 + <qeustion> + 질문+ <answer>+ 답변+ </s>'   ' 식으로 된 훈련 dataset 생성\n",
    "# 3. 모델에 input_ids, lables 을 입력하여 훈련 시킴\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "from summarizer import TransformerSummarizer\n",
    "model_path='../model/gpt-2/kogpt-2-ft-summarizer-0509/'\n",
    "#model_path='../model/gpt-2/kogpt-2/'\n",
    "#model_path='skt/kogpt2-base-v2'\n",
    "#model_path = \"gpt2-medium\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../model/gpt-2/kogpt-2-ft-summarizer-QA-0518/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-ft\", logfilename=\"../log/gpt2-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63aad3da-2f1b-4574-b907-290e54860a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A 파일 열기\n",
    "corpus_fpath = \"../korpora/korQuAD/KorQuAD_v1.0_train.csv\"\n",
    "# csv 파일 로딩해봄\n",
    "df = pd.read_csv(corpus_fpath)\n",
    "\n",
    "# df1을 list로 변환\n",
    "context_list = np.array(df['context'].tolist())\n",
    "question_list = np.array(df['question'].tolist())\n",
    "answer_list = np.array(df['answer'].tolist())\n",
    "\n",
    "# 지문<qeustion>질문<answer>답변 식으로 훈련 문장 만듬\n",
    "data_list = []\n",
    "for context, question, answer in zip(context_list, question_list, answer_list):\n",
    "    data = context + \"<question>\" + question + \"<answer>\" + answer \n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076f092e-6cee-4c3c-b8c4-f09582aec92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.<question>바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?<answer>교향곡'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707e85e0-56cb-4126-aa7e-8c0467d62eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57688\n"
     ]
    }
   ],
   "source": [
    "# 총 데이터 계수\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b017553-95a4-401a-8fb3-a3c98211f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:2249\n",
      "avg_len:122.96723755373735\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터의 평균 단어 길이를 구함\n",
    "data_len_list = [len(data.split()) for data in data_list]\n",
    "print(f'max_len:{max(data_len_list)}')\n",
    "\n",
    "# 평균 단어의 길이를 얻어옴\n",
    "avg_length = sum(data_len_list)/len(data_len_list)\n",
    "print(f'avg_len:{avg_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bdee8c0-9dc0-446a-a707-bdcef7f7a341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(data_len_list, bins=20, label='count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e17114-82ab-4e8e-aa7c-c0a65981e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 단어의 길이보다 2배정도 설정\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0154b5d0-18f0-4bea-82f0-6c2415c4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로딩 \n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06240c79-1531-4eab-a43f-5c608cbaabbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51fbc0f-ccc4-4891-aac2-0b08aebfc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9724, 14104, 9792, 10651, 405]\n",
      "[9724, 24742, 461, 9485, 405]\n",
      "***<question><answer> token len:10\n"
     ]
    }
   ],
   "source": [
    "# ** 구분자  <question><answer> 토큰의 길이를 얻어옴\n",
    "print(tokenizer.encode(\"<question>\"))\n",
    "print(tokenizer.encode(\"<answer>\"))\n",
    "extra_length = len(tokenizer.encode(\"<question>\")) + len(tokenizer.encode(\"<answer>\"))\n",
    "print(f'***<question><answer> token len:{extra_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6015caf8-b943-4897-ab70-01888598eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 설정 함수\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, max_len, my_token_len):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.sentences = sentences\n",
    "        self.my_token_len = my_token_len\n",
    "        self.result = []\n",
    "        \n",
    "        for sentence in tqdm(self.sentences):\n",
    "            # 한 문장 뒤에 </s>(EOS 토큰) 추가\n",
    "            tokenized = self.tokenizer.encode(sentence + self.eos)\n",
    "            #print(tokenized)\n",
    "            # padd \n",
    "            padded = self.pad_truncate(tokenized)\n",
    "            \n",
    "            # 출력\n",
    "            self.result.append(torch.tensor(padded))\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.result[item]\n",
    "    \n",
    "    # padd 붙이는 함수\n",
    "    def pad_truncate(self, name):\n",
    "        \n",
    "        # name_length는 총 name 길이에서 - my_token 길이를 뺀 길이가 됨\n",
    "        # (예: name 길이 = 110 이면 name_length = 110 - 8 = 102)\n",
    "        name_length = len(name) - self.my_token_len\n",
    "        \n",
    "        # name 길이 < 100 작으면, 뒤에 108개까지는 eos_id(1)로 padd 붙임\n",
    "        if name_length < self.max_len:\n",
    "            difference = self.max_len - name_length\n",
    "            result = name + [self.eos_id] * difference\n",
    "        # name 길이 > 100 크면, 100+7 까지만 name 값 출력하고, 뒤에 eos_id(1) 붙임 = 총 108개가 됨\n",
    "        elif name_length > self.max_len:\n",
    "            result = name[:self.max_len + self.my_token_len - 1]+[self.eos_id]\n",
    "        else:\n",
    "            result = name\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9086b2c1-8580-4609-b34b-9c4c857a4d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7e5c34a84d40e0b23fa327fccd5f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset 만듬\n",
    "dataset = myDataset(tokenizer, data_list, max_length, extra_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa1e5cf9-562c-43bd-9db9-8d1e387a7406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38100, 19734, 26215, 11396, 25684, 33949,  9581,  8137,  9609, 18545,\n",
       "         9022, 18150, 16022, 29090,  9558, 17600, 12341, 10526, 16158, 13580,\n",
       "         9451, 11305, 11552, 12850,  9016,  9018, 12066, 19734, 26215, 11463,\n",
       "        11491, 10503, 48195,  9021,  9145, 35197, 10181,  9054,   739,     5,\n",
       "         8135, 10142,  9131, 21837,  6903,  9137, 30294, 14159, 10560,  9803,\n",
       "         8694, 13529,  8646, 10650,  7470, 20375, 33949, 24148,  9327, 11670,\n",
       "        28453, 11927, 18520,  9322, 19533, 30624,  7098, 14669, 11292,  7426,\n",
       "        12772,  9961,  8104, 18343, 11016, 27150, 24912,  8143, 20033,  9303,\n",
       "         7623,  8137, 12904, 11404,  9244,  9883, 38067, 11889, 10285, 22524,\n",
       "        33949, 24148,  9071, 29957, 32501,  9018, 17314, 11968, 16130,  9795,\n",
       "        29333, 34583,  9362, 42122, 25712, 22316,  9093,  9480,  9755,  7185,\n",
       "         8213,  9059,  9968, 18199,  9294, 28703, 18196,  9080, 12499, 13027,\n",
       "        15048,  9301, 20717,  9137, 12273, 35445,  9199,  9554, 24912,  8143,\n",
       "        19597, 28132,  6890,  9059,  9968,  9795, 10600,  9290,  9580,  9025,\n",
       "        10960, 14927, 20033, 36387, 38100,  9148,  9130, 11192, 10219, 19533,\n",
       "        17893, 10660,  9020,  7966,  9607, 10097, 10371, 13338, 13872,  9322,\n",
       "        13682,  9474, 10342, 10408,  9258,  9018, 49214,  9147,  7966,  8168,\n",
       "         9210, 12772,  9961,  9764, 10840, 11886, 45346, 43320,  7652,  9168,\n",
       "        11630, 15123, 17150,  9442, 45821, 36902, 10305, 22124,  8135, 13544,\n",
       "         9138,  8146, 12199, 10751, 45192,  7222,  9023, 10840, 11735,  9150,\n",
       "        31543, 15026,  9496, 13712, 10554, 28483,  9405,  9135, 32574,  9022,\n",
       "        10113,  9258,  9664,  8024, 15978,  9127,  8762,  9033, 12719,  9950,\n",
       "        11082,  9038,  9731,  8747,  8146,  8185,  9325, 17893,  9033,  9030,\n",
       "        50517,  8704, 15148, 19083,  9287, 10185,  9117,  7749, 27121,  9018,\n",
       "        24607, 16346,  6866,  9036,  9199, 22958,  9277, 11955,  7235, 10960,\n",
       "          403, 14104,  9792, 10651,   405,  7596,  6947, 26215, 20033, 36387,\n",
       "        33234, 10097, 10371, 13338,  8718,     1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499222e0-0c3b-4102-ab3c-252d652560f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 생성 \n",
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04cc47a-6ad2-4550-bc51-da52fe3c8e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c62ae6ec8f499caece740b1ef8da9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb16b32f509448fb9f7039ead38f015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 08:56:14,892 - gpt2-ft - INFO - [Epoch 1/3] Iteration 360 -> Train Loss: 3.3635\n",
      "2022-05-18 08:57:32,722 - gpt2-ft - INFO - [Epoch 1/3] Iteration 720 -> Train Loss: 3.1870\n",
      "2022-05-18 08:58:50,794 - gpt2-ft - INFO - [Epoch 1/3] Iteration 1080 -> Train Loss: 3.0911\n",
      "2022-05-18 09:00:09,027 - gpt2-ft - INFO - [Epoch 1/3] Iteration 1440 -> Train Loss: 3.0134\n",
      "2022-05-18 09:01:27,147 - gpt2-ft - INFO - [Epoch 1/3] Iteration 1800 -> Train Loss: 2.9165\n",
      "2022-05-18 09:02:45,465 - gpt2-ft - INFO - [Epoch 1/3] Iteration 2160 -> Train Loss: 2.8294\n",
      "2022-05-18 09:04:04,182 - gpt2-ft - INFO - [Epoch 1/3] Iteration 2520 -> Train Loss: 2.7500\n",
      "2022-05-18 09:05:23,963 - gpt2-ft - INFO - [Epoch 1/3] Iteration 2880 -> Train Loss: 2.6942\n",
      "2022-05-18 09:06:43,703 - gpt2-ft - INFO - [Epoch 1/3] Iteration 3240 -> Train Loss: 2.6381\n",
      "2022-05-18 09:08:02,737 - gpt2-ft - INFO - [Epoch 1/3] Iteration 3600 -> Train Loss: 2.5724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa80d252dd414290921c5e3f468446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 09:09:21,772 - gpt2-ft - INFO - [Epoch 2/3] Iteration 3960 -> Train Loss: 2.4063\n",
      "2022-05-18 09:10:40,536 - gpt2-ft - INFO - [Epoch 2/3] Iteration 4320 -> Train Loss: 2.3737\n",
      "2022-05-18 09:11:59,886 - gpt2-ft - INFO - [Epoch 2/3] Iteration 4680 -> Train Loss: 2.3379\n",
      "2022-05-18 09:13:18,578 - gpt2-ft - INFO - [Epoch 2/3] Iteration 5040 -> Train Loss: 2.3050\n",
      "2022-05-18 09:14:36,967 - gpt2-ft - INFO - [Epoch 2/3] Iteration 5400 -> Train Loss: 2.2579\n",
      "2022-05-18 09:14:40,182 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/kogpt-2-ft-summarizer-QA-0518/batch:16-ep:3-lr:0.000030000-5m18d-9:14\n",
      "2022-05-18 09:15:57,642 - gpt2-ft - INFO - [Epoch 2/3] Iteration 5760 -> Train Loss: 2.2250\n",
      "2022-05-18 09:17:17,393 - gpt2-ft - INFO - [Epoch 2/3] Iteration 6120 -> Train Loss: 2.1905\n",
      "2022-05-18 09:18:36,218 - gpt2-ft - INFO - [Epoch 2/3] Iteration 6480 -> Train Loss: 2.1601\n",
      "2022-05-18 09:19:55,681 - gpt2-ft - INFO - [Epoch 2/3] Iteration 6840 -> Train Loss: 2.1297\n",
      "2022-05-18 09:21:14,924 - gpt2-ft - INFO - [Epoch 2/3] Iteration 7200 -> Train Loss: 2.1003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f9fdbb07294b35a979d28049ca7a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 09:22:33,448 - gpt2-ft - INFO - [Epoch 3/3] Iteration 7560 -> Train Loss: 1.9896\n",
      "2022-05-18 09:23:52,652 - gpt2-ft - INFO - [Epoch 3/3] Iteration 7920 -> Train Loss: 1.9859\n",
      "2022-05-18 09:25:11,340 - gpt2-ft - INFO - [Epoch 3/3] Iteration 8280 -> Train Loss: 1.9682\n",
      "2022-05-18 09:26:30,500 - gpt2-ft - INFO - [Epoch 3/3] Iteration 8640 -> Train Loss: 1.9388\n",
      "2022-05-18 09:27:49,405 - gpt2-ft - INFO - [Epoch 3/3] Iteration 9000 -> Train Loss: 1.9316\n",
      "2022-05-18 09:29:08,795 - gpt2-ft - INFO - [Epoch 3/3] Iteration 9360 -> Train Loss: 1.9069\n",
      "2022-05-18 09:30:28,347 - gpt2-ft - INFO - [Epoch 3/3] Iteration 9720 -> Train Loss: 1.9055\n",
      "2022-05-18 09:31:47,698 - gpt2-ft - INFO - [Epoch 3/3] Iteration 10080 -> Train Loss: 1.8947\n",
      "2022-05-18 09:33:07,255 - gpt2-ft - INFO - [Epoch 3/3] Iteration 10440 -> Train Loss: 1.8945\n",
      "2022-05-18 09:34:26,579 - gpt2-ft - INFO - [Epoch 3/3] Iteration 10800 -> Train Loss: 1.8911\n",
      "2022-05-18 09:34:31,116 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/kogpt-2-ft-summarizer-QA-0518/batch:16-ep:3-lr:0.000030000-5m18d-9:34\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "##################################################\n",
    "epochs = 3            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "\n",
    "total_loss = 0\n",
    "list_train_loss = []\n",
    "\n",
    "# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "model.zero_grad(set_to_none=True)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data.to(device)\n",
    "        labels = data.to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # logits_shape: torch.Size([32, 68, 51200])\n",
    "        # => batch_size, sequence_max_len, token_len\n",
    "        #print(f'logits_shape: {logits.shape}')                    \n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            #correct = AccuracyForMLM(logits, labels, attention_mask)           \n",
    "            #total_correct += correct.sum().item() \n",
    "            #total_len += attention_mask.sum().item()\n",
    "            #=========================================\n",
    "     \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                                   \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}'.format(epoch+1, epochs, itr, train_loss))\n",
    "                     \n",
    "                list_train_loss.append(train_loss)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                ####################################################################\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03d6fbf-bc8a-49da-b2b3-732654e1fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 09:34:33,141 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/kogpt-2-ft-summarizer-QA-0518/batch:16-ep:3-lr:0.000030000-5m18d-9:34\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2e3f43-a44f-4d02-b1a3-289a4d7e727c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5bebde9-ddec-48b8-b822-19e40967a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론시 topk 알고리즘 사용\n",
    "def topk(probs, n=9):\n",
    "    probs = torch.softmax(probs, dim=-1)\n",
    "    \n",
    "    tokensProb, topIx = torch.topk(probs, k=n)\n",
    "    tokensProb = tokensProb / torch.sum(tokensProb)\n",
    "    \n",
    "    tokensProb = tokensProb.cpu().detach().numpy()\n",
    "    \n",
    "    choice = np.random.choice(n,1,p=tokensProb)\n",
    "    tokenId = topIx[choice][0]\n",
    "    \n",
    "    return int(tokenId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568582f6-de63-43f0-8310-6126b4e07165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize 추론 처리 함수 \n",
    "def model_infer(model, tokenizer, review, max_length=15):\n",
    "    review_encoded = tokenizer.encode(review)\n",
    "    result = review_encoded\n",
    "    initial_input = torch.tensor(review_encoded).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(initial_input)\n",
    "        \n",
    "        logits = outputs.logits[0,-1]\n",
    "        print(logits.shape)  # embedding 계수 출력됨\n",
    "        #result.append(topk(logits))\n",
    "        #print(result)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            input = torch.tensor(result).unsqueeze(0).to(device)\n",
    "            outputs = model(input)\n",
    "            logits = outputs.logits[0,-1]\n",
    "            res_id = topk(logits)\n",
    "            \n",
    "            if res_id == tokenizer.eos_token_id:\n",
    "                return tokenizer.decode(result)\n",
    "            else:\n",
    "                result.append(res_id)\n",
    "                \n",
    "    return tokenizer.decode(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e396e2d-a08b-4a83-b18f-b1a3b0c53d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 전남도에서 추진하는 사업은?\n",
      "torch.Size([51200])\n",
      "A:신재생에너지 보급주택지원사업\n",
      "\n",
      "\n",
      "Q: 사업 담당 부서는?\n",
      "torch.Size([51200])\n",
      "A:광주전남\n",
      "\n",
      "\n",
      "Q: 전기요금 절감 금액은?\n",
      "torch.Size([51200])\n",
      "A:160만원\n",
      "\n",
      "\n",
      "Q: 지원 가구수는?\n",
      "torch.Size([51200])\n",
      "A:110 가구\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A review is initially fed to the model.\n",
    "# A choice from the top-k choices is selected.\n",
    "# The choice is added to the summary and the current sequence is fed to the model.\n",
    "# Repeat steps 2 and 3 until either max_len is achieved or the EOS token is generated.\n",
    "\n",
    "context = '''\n",
    "전남도는 청정하고 안전한 신재생에너지 보급 확산을 위해 추진하는 ‘2022년 신재생에너지 보급 주택지원사업’ 참여 가구를 9일부터 6월 3일까지 모집한다고 밝혔다.\n",
    "신재생에너지 보급 주택지원사업은 주택에 태양광, 태양열, 연료전지, 지열 등 신재생에너지 설비를 설치하는 도민에게 정부 지원금 외에 도비와 시군비를 추가로 지원하는 사업이다.\n",
    "사업 대상자로 선정되면 설치비 자부담분(50%)의 40%를 도비와 시군비로 지원하기 때문에 비용부담을 줄일 수 있다.\n",
    "전남도는 올해 지방비 17억원을 들여 1600여 가구에 신재생에너지 설비를 보급할 계획이다.\n",
    "지원을 바라는 주택 소유자는 한국에너지공단 그린홈 누리집에서 회원가입 후 공단에 등록한 참여업체를 지정해 신청하면 된다.\n",
    "지방비 보조금은 한국에너지공단의 최종 사업 승인 후 해당 시군에 지원 신청을 하면 예산 범위에서 선착순 지원한다.\n",
    "자세한 사항은 한국에너지공단 광주전남지역본부, 전남도 에너지신산업과, 시군 에너지업무 담당 부서로 문의하면 된다.\n",
    "주택에 3KW 태양광 설비를 설치하면 총 설치비 516만원 중 국비 258만원과 추가로 지방비 103만원을 지원하기 때문에 155만원만 자부담하면 된다.\n",
    "3KW 태양광 설비를 설치한 가구는 월 4만5000원씩 연간 54만원의 전기요금을 절감할 수 있다.\n",
    "'''\n",
    "questions = ['전남도에서 추진하는 사업은?',\n",
    "             '사업 담당 부서는?',\n",
    "             '전기요금 절감 금액은?',\n",
    "             '지원 가구수는?']\n",
    "\n",
    "for question in questions:\n",
    "    \n",
    "    print(f'Q: {question}')\n",
    "    inputs = context + \"<question>\" + question + \"<answer>\"\n",
    "    \n",
    "    output = model_infer(model, tokenizer, inputs , 100)\n",
    "    answer = output.split(\"<answer>\")[1].strip()\n",
    "    print(f'A:{answer}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b2e96-3990-4a8a-b166-6a46e3564d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
