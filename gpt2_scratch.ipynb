{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2a200f-1242-4810-99b7-2a5223393bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../log/bwdataset_2022-05-18.log\n",
      "logfilepath:../log/qnadataset_2022-05-18.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-scratch_2022-05-18.log\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# gpt2 모델을 새롭게 만드는 예제\n",
    "#\n",
    "# [과정]\n",
    "\n",
    "# 1. Sentencepiece tokenizer 생성\n",
    "# => 생성은 toeknzier/new_token.ipynb 참조\n",
    "# => 여기서는 만들어진 tokenizer를 사용 함\n",
    "#\n",
    "# 2. 빈껍데기 GPT-2 모델 생성 \n",
    "# => **반드시 위 vocab_size와 같은 크기로 word_embedding 사이즈 설정해야 함\n",
    "#\n",
    "# 3. 훈련\n",
    "# => vocab 만들때 동일한 말뭉치를 훈련 데이터로 사용\n",
    "#\n",
    "# 4. 모델과 tokenizer 저장\n",
    "\n",
    "#=====================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import GPT2Config, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, TextGeneration_Dataset\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-scratch\", logfilename=\"../log/gpt2-scratch\")\n",
    "\n",
    "model_path = '../model/gpt-2/mymodel'\n",
    "OUTPATH = '../model/gpt-2/mymodel_scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dacf2a4-ca47-4a42-ad6d-482525ec09a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*vocab_size:10661\n"
     ]
    }
   ],
   "source": [
    "# 1. Sentencepiece tokenizer 로딩 \n",
    "# => 반드시 bos_token, eos_token, unk_token, pad_token, mask_token 들은 tokenizer 생성할때 사용한 vocab을 지정해야 함\n",
    "# => 생성은 toeknzier/new_token.ipynb 참조\n",
    "# => 여기서는 만들어진 tokenizer를 사용 함\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='<cls>',\n",
    "                                                   eos_token='<eos>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(f'*vocab_size:{vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5c9208-4962-4683-8630-65ff8ece8259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2448, 703, 7074, 120, 2309, 271]\n",
      "오늘은 날씨가 좋다\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 테스트 \n",
    "sentence = \"오늘은 날씨가 좋다\"\n",
    "encode = tokenizer.encode(sentence)\n",
    "print(encode)\n",
    "decode = tokenizer.decode(encode)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665a03ae-a6f3-46f0-917a-489c0c8c8c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94030080\n"
     ]
    }
   ],
   "source": [
    "# 2. 빈껍데기 GPT-2 모델 생성 \n",
    "# => **반드시 위 vocab_size와 같은 크기로 word_embedding 사이즈 설정해야 함\n",
    "configuration = GPT2Config(vocab_size=vocab_size)\n",
    "model = GPT2LMHeadModel(config=configuration) \n",
    "model.to(device)\n",
    "print(model.num_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80dc8096-7ee0-43eb-bb03-803371d9ec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(10661, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=10661, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vocab_size가 잘 설정되었는지 모델 출력 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a666cc-3eaa-4a95-abf8-28f8e1e84944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f3af00561b4c77a0a5af673066a41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.', '카터는 이집트와 이스라엘을 조정하여, 캠프 데이비드에서 안와르 사다트 대통령과 메나헴 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 체결했다.', '그러나 이것은 공화당과 미국의 유대인 단체의 반발을 일으켰다.', '1979년 백악관에서 양국 간의 평화조약으로 이끌어졌다.', '또한 소련과 제2차 전략 무기 제한 협상에 조인했다.']\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# 3.훈련\n",
    "# => vocab 만들때 동일한 말뭉치를 불러옴 \n",
    "corpus_path = \"../korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "all_sentences = []\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "      for line in tqdm(f):\n",
    "            all_sentences.append(line.strip())  # strip() 계행문자 제거\n",
    "            \n",
    "print(all_sentences[10:15])\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "667213f6-fb2b-4d10-a0b0-4ca85b3997b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca28c1df6f747ac8e74d27e093794e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token_len:180\n"
     ]
    }
   ],
   "source": [
    "# 최대 토큰 계수를 구함.\n",
    "max_token_len = max([len(tokenizer.encode(s)) for s in tqdm(all_sentences)])\n",
    "print(f'max_token_len:{max_token_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafcda9b-100f-45c7-a50f-fcc096a606a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d82df8faa104f8ba6e3c8c970411bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([   0, 5494, 3122, 1277, 9989, 5260,  168, 3183, 3360, 3814, 5100, 1139,\n",
      "         539, 1261, 1554,    1,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4]), tensor([    0, 10574,  5696,  5326,  1008,   590,   343,   651,   704,  7479,\n",
      "         1559,  7095,  9879,   522,  4142,  1288,   669,   405,  1015,   271,\n",
      "          890,  1191,   160,  1716,   214,  1740,   188,  2083,   160,  1358,\n",
      "         9518,  1199,  2017,  1558,  7095,  5276,   522,   316,  8758,  3887,\n",
      "          948,  1554,     1,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4]), tensor([   0, 1262, 2197, 3360, 5275, 2093, 7969, 1137, 1574, 4841,  704, 8897,\n",
      "        1554,    1,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4])], [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])])\n"
     ]
    }
   ],
   "source": [
    "# TextGenerattion 데이터셋 생성\n",
    "# => bos_token + 문장 + eos_token\n",
    "dataset = TextGeneration_Dataset(all_sentences, tokenizer, max_length=max_token_len)\n",
    "print(dataset[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf8e7bc-fe4c-4894-ab48-4c62d1c22167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:9900\n",
      "val_size:100\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 test 데이터를 나눔\n",
    "train_size = int(0.99 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'train_size:{len(train_set)}')\n",
    "print(f'val_size:{len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efcfcae-a802-49f7-8264-a631c3309def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[    0,  1138,  1732,  ...,     4,     4,     4],\n",
      "        [    0,  1673,   405,  ...,     4,     4,     4],\n",
      "        [    0,  3061,  1687,  ...,     4,     4,     4],\n",
      "        ...,\n",
      "        [    0,  2460,  4116,  ...,     4,     4,     4],\n",
      "        [    0,  1094,   790,  ...,     4,     4,     4],\n",
      "        [    0, 10648,   707,  ...,     4,     4,     4]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "# dataloader 생성\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, sampler = RandomSampler(train_set), batch_size=batch_size)\n",
    "eval_loader = DataLoader(val_set, sampler = RandomSampler(val_set), batch_size=batch_size)\n",
    "\n",
    "for val_data in eval_loader:\n",
    "    print(val_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5765817-c8a0-4ea0-bcd5-7e16adafdd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cls> 즉,<eos>\n"
     ]
    }
   ],
   "source": [
    "decode = tokenizer.decode([0,1406, 1])\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b09538-fb07-4764-a9ac-fa8a7c2f8dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f656bd0-7bd1-486b-adc4-9a1be287d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbdee9092094ceaa1fbbc9752988bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1542695c21465892b26a0fdc20b6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f183473cc54639ad390be73d0b6188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:39:12,199 - gpt2-scratch - INFO - [Epoch 1/10] Iteration 310 -> Train Loss: 1.4370, Val Loss: 1.2207323014736176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a645ad7f0bdc48ce8f73bca97eefe198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ebc93e5f1b42018df15393afe96443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:40:21,170 - gpt2-scratch - INFO - [Epoch 2/10] Iteration 620 -> Train Loss: 1.0132, Val Loss: 1.0779387801885605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d96db82297043c7bf28c654edd3bf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cf9683badf4825b18ab9c1f05e3336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:41:31,947 - gpt2-scratch - INFO - [Epoch 3/10] Iteration 930 -> Train Loss: 0.9821, Val Loss: 0.9797493815422058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871162d0af80474694a87f508eb99128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2979d4e26a5464a92e312bf132e4863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:42:45,522 - gpt2-scratch - INFO - [Epoch 4/10] Iteration 1240 -> Train Loss: 0.9445, Val Loss: 1.0073799788951874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6b7deb38ec489d8be7837188b7159a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dba8fd3eac48e1961774d834b734ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:43:58,239 - gpt2-scratch - INFO - [Epoch 5/10] Iteration 1550 -> Train Loss: 0.9085, Val Loss: 1.0001187324523926\n",
      "2022-05-18 13:43:58,763 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/mymodel_scratchbatch:32-ep:10-lr:0.000030000-5m18d-13:43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf9f8dd327d4fe798ad08ee7076576d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437cd210d2de4793b111970c57c57812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:45:10,983 - gpt2-scratch - INFO - [Epoch 6/10] Iteration 1860 -> Train Loss: 0.8781, Val Loss: 0.9615229517221451\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e928469276c42568551afc67102e5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c770406c0634696b88d78dd8bb6579d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:46:22,928 - gpt2-scratch - INFO - [Epoch 7/10] Iteration 2170 -> Train Loss: 0.8530, Val Loss: 0.9699893593788147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f5929c903e4c319b89a66edd41a5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8ca34a18a245be8d4d0294b988ea60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:47:31,896 - gpt2-scratch - INFO - [Epoch 8/10] Iteration 2480 -> Train Loss: 0.8329, Val Loss: 0.9173761606216431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d7fd266ad147f38a65058914b525f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5f835b0b8c49cb97302a707deb999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:48:41,169 - gpt2-scratch - INFO - [Epoch 9/10] Iteration 2790 -> Train Loss: 0.8169, Val Loss: 0.9672838449478149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f52842196ca4339bddbda9401df2862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84193d5e068b4c2c8c7270e6cc772263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:49:50,875 - gpt2-scratch - INFO - [Epoch 10/10] Iteration 3100 -> Train Loss: 0.8054, Val Loss: 1.011869192123413\n",
      "2022-05-18 13:49:51,375 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/mymodel_scratchbatch:32-ep:10-lr:0.000030000-5m18d-13:49\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "##################################################\n",
    "epochs = 10            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(total_steps*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "\n",
    "total_loss = 0\n",
    "total_test_len = 0\n",
    "total_test_loss = 0\n",
    "total_test_loss_count = 0\n",
    "\n",
    "list_train_loss = []\n",
    "list_validation_loss = []\n",
    "\n",
    "# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "model.zero_grad(set_to_none=True)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data[0].to(device)\n",
    "        attention_mask = data[1].to(device)\n",
    "        \n",
    "         # labels은 input_ids와 동일하게 입력 (*GPT2LMHeadModel 을 이용하는 경우, 내부적으로 labels 값에 대해 shift 연산처리를 해서 손실 구함)\n",
    "        labels = data[0].to(device)  \n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # logits_shape: torch.Size([32, 68, 51200])\n",
    "        # => batch_size, sequence_max_len, token_len\n",
    "        #print(f'logits_shape: {logits.shape}')                    \n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                #train_acc = total_correct/total_len\n",
    "                \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "                for data in tqdm(eval_loader):\n",
    "                #for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data[0].to(device)\n",
    "                    attention_mask = data[1].to(device)\n",
    "                    labels = data[0].to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "                        \n",
    "                        total_test_loss += loss.item()\n",
    "                        total_test_loss_count += 1\n",
    "                        \n",
    "                val_loss = total_test_loss/total_test_loss_count\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Val Loss: {}'.format(epoch+1, epochs, itr, train_loss, val_loss))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_validation_loss.append(val_loss)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_test_loss = 0\n",
    "                total_test_len = 0\n",
    "                total_test_loss_count = 0\n",
    "                ####################################################################\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c89f6927-f298-4f59-b05a-c683e565367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 13:49:51,972 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/mymodel_scratchbatch:32-ep:10-lr:0.000030000-5m18d-13:49\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f46279-f19e-4861-8c33-677ca13ea5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "plt.plot(list_validation_loss, label='val Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
