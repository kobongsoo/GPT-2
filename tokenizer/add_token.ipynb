{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d29c6a55-3fee-4efa-a0d5-df4dee451324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================\n",
    "#기존 Kogpt-2 toeknizer 사전에 새로운 토큰들을 추가하는 예제임\n",
    "#\n",
    "# [토큰 추가 과정]\n",
    "# 1) 기존 tokenizer 불러옴\n",
    "# 2) add_tokens 함수 이용 해서, 신규 token들 추가\n",
    "# 3) 추가된 tokenizer를 새롭게 저장\n",
    "#\n",
    "#=========================================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8d0f005-df90-4215-ab71-90a253564ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*기존 vocab 수: 51200\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 불러옴\n",
    "# => PreTrainedTokenizerFase 혹은 GPT2TokenizerFast 둘중 아무거나 사용해도 됨.\n",
    "model_path='../../model/gpt-2/kogpt-2/'\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n",
    "'''\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path,\n",
    "                                            bos_token='</s>',\n",
    "                                            eos_token='</s>',\n",
    "                                            unk_token='<unk>',\n",
    "                                            pad_token='<pad>',\n",
    "                                            mask_token='<mask>')\n",
    "'''\n",
    "\n",
    "print(f'*기존 vocab 수: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d64bbf23-9613-4d2b-bf17-413613598dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*기존 vocab 수: 51202\n"
     ]
    }
   ],
   "source": [
    "print(f'*기존 vocab 수: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ae29d1-da4e-4e56-a8e6-bc941731284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 token 추가함.\n",
    "new_vocab = ['<question>', '<answer>'] # 추가할 token 들\n",
    "new_tokenizer = tokenizer.add_tokens(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41952fb-286f-4a27-af16-e60a8b6f47e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*추가후 vocab 수: 51202\n"
     ]
    }
   ],
   "source": [
    "print(f'*추가후 vocab 수: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dac691d-0eca-4d0a-bd45-693eb9a6df2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../model/gpt-2/kogpt-2/addvocab/tokenizer_config.json',\n",
       " '../../model/gpt-2/kogpt-2/addvocab/special_tokens_map.json',\n",
       " '../../model/gpt-2/kogpt-2/addvocab/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#추가한 tokenizer 저장\n",
    "import os\n",
    "OUT_PATH = '../../model/gpt-2/kogpt-2/addvocab'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69ae673-bf41-4a9d-ac21-6a8c928dd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*기존 vocab 수: 51202\n"
     ]
    }
   ],
   "source": [
    "# 추가한 VOCAB을 불러옴\n",
    "tokenizer1 = PreTrainedTokenizerFast.from_pretrained(OUT_PATH,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n",
    "print(f'*기존 vocab 수: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13afeea5-a1e2-4bce-b593-6acfe9f39076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51201]\n"
     ]
    }
   ],
   "source": [
    "# encode로 token -> tokenid 변환\n",
    "tokenid = tokenizer1.encode('<answer>')\n",
    "print(tokenid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc475620-fab7-4897-857f-55e4551bc564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<answer>\n"
     ]
    }
   ],
   "source": [
    "# decode로 tokeniid->token으로 변환\n",
    "token = tokenizer1.decode(51201)\n",
    "print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
