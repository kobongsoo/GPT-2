{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2a200f-1242-4810-99b7-2a5223393bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../log/bwdataset_2022-05-11.log\n",
      "logfilepath:../log/qnadataset_2022-05-11.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-scratch_2022-05-11.log\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# gpt2 모델을 새롭게 만드는 예제\n",
    "#\n",
    "# [과정]\n",
    "\n",
    "# 1. Sentencepiece tokenizer 생성\n",
    "# => 생성은 toeknzier/new_token.ipynb 참조\n",
    "# => 여기서는 만들어진 tokenizer를 사용 함\n",
    "#\n",
    "# 2. 빈껍데기 GPT-2 모델 생성 \n",
    "# => **반드시 위 vocab_size와 같은 크기로 word_embedding 사이즈 설정해야 함\n",
    "#\n",
    "# 3. 훈련\n",
    "# => vocab 만들때 동일한 말뭉치를 훈련 데이터로 사용\n",
    "#\n",
    "# 4. 모델과 tokenizer 저장\n",
    "\n",
    "#=====================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import GPT2Config, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, TextGeneration_Dataset\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-scratch\", logfilename=\"../log/gpt2-scratch\")\n",
    "\n",
    "model_path = '../model/gpt-2/mymodel'\n",
    "OUTPATH = '../model/gpt-2/mymodel1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dacf2a4-ca47-4a42-ad6d-482525ec09a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*vocab_size:10661\n"
     ]
    }
   ],
   "source": [
    "# 1. Sentencepiece tokenizer 로딩 \n",
    "# => 반드시 bos_token, eos_token, unk_token, pad_token, mask_token 들은 tokenizer 생성할때 사용한 vocab을 지정해야 함\n",
    "# => 생성은 toeknzier/new_token.ipynb 참조\n",
    "# => 여기서는 만들어진 tokenizer를 사용 함\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='<cls>',\n",
    "                                                   eos_token='<eos>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(f'*vocab_size:{vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5c9208-4962-4683-8630-65ff8ece8259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2448, 703, 7074, 120, 2309, 271]\n",
      "오늘은 날씨가 좋다\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 테스트 \n",
    "sentence = \"오늘은 날씨가 좋다\"\n",
    "encode = tokenizer.encode(sentence)\n",
    "print(encode)\n",
    "decode = tokenizer.decode(encode)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a03ae-a6f3-46f0-917a-489c0c8c8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 빈껍데기 GPT-2 모델 생성 \n",
    "# => **반드시 위 vocab_size와 같은 크기로 word_embedding 사이즈 설정해야 함\n",
    "configuration = GPT2Config(vocab_size=vocab_size)\n",
    "model = GPT2LMHeadModel(config=configuration) \n",
    "model.to(device)\n",
    "print(model.num_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc8096-7ee0-43eb-bb03-803371d9ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size가 잘 설정되었는지 모델 출력 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a666cc-3eaa-4a95-abf8-28f8e1e84944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d4272cfe214460a47e4dffc70cfff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.', '카터는 이집트와 이스라엘을 조정하여, 캠프 데이비드에서 안와르 사다트 대통령과 메나헴 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 체결했다.', '그러나 이것은 공화당과 미국의 유대인 단체의 반발을 일으켰다.', '1979년 백악관에서 양국 간의 평화조약으로 이끌어졌다.', '또한 소련과 제2차 전략 무기 제한 협상에 조인했다.']\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# 3.훈련\n",
    "# => vocab 만들때 동일한 말뭉치를 불러옴 \n",
    "corpus_path = \"../korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "all_sentences = []\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "      for line in tqdm(f):\n",
    "            all_sentences.append(line.strip())  # strip() 계행문자 제거\n",
    "            \n",
    "print(all_sentences[10:15])\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667213f6-fb2b-4d10-a0b0-4ca85b3997b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2bbe8ea42440daa14466e224af3ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token_len:180\n"
     ]
    }
   ],
   "source": [
    "# 최대 토큰 계수를 구함.\n",
    "max_token_len = max([len(tokenizer.encode(s)) for s in tqdm(all_sentences)])\n",
    "print(f'max_token_len:{max_token_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eafcda9b-100f-45c7-a50f-fcc096a606a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2036b130cf4bc8be226c8e70fcaefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([   0, 5494, 3122, 1277, 9989, 5260,  168, 3183, 3360, 3814, 5100, 1139,\n",
      "         539, 1261, 1554,    1,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4]), tensor([    0, 10574,  5696,  5326,  1008,   590,   343,   651,   704,  7479,\n",
      "         1559,  7095,  9879,   522,  4142,  1288,   669,   405,  1015,   271,\n",
      "          890,  1191,   160,  1716,   214,  1740,   188,  2083,   160,  1358,\n",
      "         9518,  1199,  2017,  1558,  7095,  5276,   522,   316,  8758,  3887,\n",
      "          948,  1554,     1,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4]), tensor([   0, 1262, 2197, 3360, 5275, 2093, 7969, 1137, 1574, 4841,  704, 8897,\n",
      "        1554,    1,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4])], [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])])\n"
     ]
    }
   ],
   "source": [
    "# TextGenerattion 데이터셋 생성\n",
    "# => bos_token + 문장 + eos_token\n",
    "dataset = TextGeneration_Dataset(all_sentences, tokenizer, max_length=max_token_len)\n",
    "print(dataset[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8e7bc-fe4c-4894-ab48-4c62d1c22167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 test 데이터를 나눔\n",
    "train_size = int(0.99 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'train_size:{len(train_set)}')\n",
    "print(f'val_size:{len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcfcae-a802-49f7-8264-a631c3309def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 생성\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, sampler = RandomSampler(train_set), batch_size=batch_size)\n",
    "eval_loader = DataLoader(val_set, sampler = RandomSampler(val_set), batch_size=batch_size)\n",
    "\n",
    "for val_data in eval_loader:\n",
    "    print(val_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5765817-c8a0-4ea0-bcd5-7e16adafdd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = tokenizer.decode([0,1406, 1])\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b09538-fb07-4764-a9ac-fa8a7c2f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f656bd0-7bd1-486b-adc4-9a1be287d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "##################################################\n",
    "epochs = 2            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "\n",
    "total_loss = 0\n",
    "total_test_len = 0\n",
    "total_test_loss = 0\n",
    "total_test_loss_count = 0\n",
    "\n",
    "list_train_loss = []\n",
    "list_validation_loss = []\n",
    "\n",
    "# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "model.zero_grad(set_to_none=True)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data[0].to(device)\n",
    "        attention_mask = data[1].to(device)\n",
    "        \n",
    "         # labels은 input_ids와 동일하게 입력 (*GPT2LMHeadModel 을 이용하는 경우, 내부적으로 labels 값에 대해 shift 연산처리를 해서 손실 구함)\n",
    "        labels = data[0].to(device)  \n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # logits_shape: torch.Size([32, 68, 51200])\n",
    "        # => batch_size, sequence_max_len, token_len\n",
    "        #print(f'logits_shape: {logits.shape}')                    \n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                #train_acc = total_correct/total_len\n",
    "                \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "                for data in tqdm(eval_loader):\n",
    "                #for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data[0].to(device)\n",
    "                    attention_mask = data[1].to(device)\n",
    "                    labels = data[0].to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "                        \n",
    "                        total_test_loss += loss.item()\n",
    "                        total_test_loss_count += 1\n",
    "                        \n",
    "                val_loss = total_test_loss/total_test_loss_count\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Val Loss: {}'.format(epoch+1, epochs, itr, train_loss, val_loss))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_validation_loss.append(val_loss)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_test_loss = 0\n",
    "                total_test_len = 0\n",
    "                total_test_loss_count = 0\n",
    "                ####################################################################\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f6927-f298-4f59-b05a-c683e565367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f46279-f19e-4861-8c33-677ca13ea5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "plt.plot(list_validation_loss, label='val Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
