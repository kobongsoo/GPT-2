{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../log/bwdataset_2022-05-10.log\n",
      "logfilepath:../log/qnadataset_2022-05-10.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-ft_2022-05-10.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# kogpt2 를 이용해 훈련한 Q&A 훈련 예시\n",
    "#\n",
    "# => train loss, val loss 만 구함\n",
    "#\n",
    "# => 여기서는 훈련할때 <question>, <answer> 2개의 구분자 토큰을 지정하였음. \n",
    "# [훈련 dataset]\n",
    "# => input_ids = '지문<qeustion>질문<answer>답변</s>'   \n",
    "# => labels = input_ids와 동일\n",
    "#\n",
    "# [Q&A 훈련 과정]\n",
    "# \n",
    "# 1. gpt-2 모델 선언(GPT2LMHeadModel), tokenizer 선언(PreTrainedTokenizerFast)\n",
    "# 2. '지문 + <qeustion> + 질문+ <answer>+ 답변+ </s>'   ' 식으로 된 훈련 dataset 생성\n",
    "# 3. 모델에 input_ids, lables 을 입력하여 훈련 시킴\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "from summarizer import TransformerSummarizer\n",
    "model_path='../model/gpt-2/kogpt-2-ft-summarizer-0509/'\n",
    "#model_path='../model/gpt-2/kogpt-2/'\n",
    "#model_path='skt/kogpt2-base-v2'\n",
    "#model_path = \"gpt2-medium\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../model/gpt-2/kogpt-2-ft-summarizer-QA-0510/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-ft\", logfilename=\"../log/gpt2-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63aad3da-2f1b-4574-b907-290e54860a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A 파일 열기\n",
    "corpus_fpath = \"../korpora/korQuAD/KorQuAD_v1.0_train.csv\"\n",
    "# csv 파일 로딩해봄\n",
    "df = pd.read_csv(corpus_fpath)\n",
    "\n",
    "# df1을 list로 변환\n",
    "context_list = np.array(df['context'].tolist())\n",
    "question_list = np.array(df['question'].tolist())\n",
    "answer_list = np.array(df['answer'].tolist())\n",
    "\n",
    "# 지문<qeustion>질문<answer>답변 식으로 훈련 문장 만듬\n",
    "data_list = []\n",
    "for context, question, answer in zip(context_list, question_list, answer_list):\n",
    "    data = context + \"<question>\" + question + \"<answer>\" + answer \n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076f092e-6cee-4c3c-b8c4-f09582aec92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.<question>바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?<answer>교향곡'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707e85e0-56cb-4126-aa7e-8c0467d62eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57688\n"
     ]
    }
   ],
   "source": [
    "# 총 데이터 계수\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b017553-95a4-401a-8fb3-a3c98211f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:2249\n",
      "avg_len:122.96723755373735\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터의 평균 단어 길이를 구함\n",
    "data_len_list = [len(data.split()) for data in data_list]\n",
    "print(f'max_len:{max(data_len_list)}')\n",
    "\n",
    "# 평균 단어의 길이를 얻어옴\n",
    "avg_length = sum(data_len_list)/len(data_len_list)\n",
    "print(f'avg_len:{avg_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdee8c0-9dc0-446a-a707-bdcef7f7a341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(data_len_list, bins=20, label='count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e17114-82ab-4e8e-aa7c-c0a65981e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 단어의 길이보다 2배정도 설정\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0154b5d0-18f0-4bea-82f0-6c2415c4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로딩 \n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06240c79-1531-4eab-a43f-5c608cbaabbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51fbc0f-ccc4-4891-aac2-0b08aebfc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9724, 14104, 9792, 10651, 405]\n",
      "[9724, 24742, 461, 9485, 405]\n",
      "***<question><answer> token len:10\n"
     ]
    }
   ],
   "source": [
    "# ** 구분자  <question><answer> 토큰의 길이를 얻어옴\n",
    "print(tokenizer.encode(\"<question>\"))\n",
    "print(tokenizer.encode(\"<answer>\"))\n",
    "extra_length = len(tokenizer.encode(\"<question>\")) + len(tokenizer.encode(\"<answer>\"))\n",
    "print(f'***<question><answer> token len:{extra_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6015caf8-b943-4897-ab70-01888598eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 설정 함수\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, max_len, my_token_len):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.sentences = sentences\n",
    "        self.my_token_len = my_token_len\n",
    "        self.result = []\n",
    "        \n",
    "        for sentence in tqdm(self.sentences):\n",
    "            # 한 문장 뒤에 </s>(EOS 토큰) 추가\n",
    "            tokenized = self.tokenizer.encode(sentence + self.eos)\n",
    "            #print(tokenized)\n",
    "            # padd \n",
    "            padded = self.pad_truncate(tokenized)\n",
    "            \n",
    "            # 출력\n",
    "            self.result.append(torch.tensor(padded))\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.result[item]\n",
    "    \n",
    "    # padd 붙이는 함수\n",
    "    def pad_truncate(self, name):\n",
    "        \n",
    "        # name_length는 총 name 길이에서 - my_token 길이를 뺀 길이가 됨\n",
    "        # (예: name 길이 = 110 이면 name_length = 110 - 8 = 102)\n",
    "        name_length = len(name) - self.my_token_len\n",
    "        \n",
    "        # name 길이 < 100 작으면, 뒤에 108개까지는 eos_id(1)로 padd 붙임\n",
    "        if name_length < self.max_len:\n",
    "            difference = self.max_len - name_length\n",
    "            result = name + [self.eos_id] * difference\n",
    "        # name 길이 > 100 크면, 100+7 까지만 name 값 출력하고, 뒤에 eos_id(1) 붙임 = 총 108개가 됨\n",
    "        elif name_length > self.max_len:\n",
    "            result = name[:self.max_len + self.my_token_len - 1]+[self.eos_id]\n",
    "        else:\n",
    "            result = name\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086b2c1-8580-4609-b34b-9c4c857a4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 만듬\n",
    "dataset = myDataset(tokenizer, data_list, max_length, extra_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e5cf9-562c-43bd-9db9-8d1e387a7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499222e0-0c3b-4102-ab3c-252d652560f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 생성 \n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04cc47a-6ad2-4550-bc51-da52fe3c8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "##################################################\n",
    "epochs = 2            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "\n",
    "total_loss = 0\n",
    "list_train_loss = []\n",
    "\n",
    "# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "model.zero_grad(set_to_none=True)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data.to(device)\n",
    "        labels = data.to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # logits_shape: torch.Size([32, 68, 51200])\n",
    "        # => batch_size, sequence_max_len, token_len\n",
    "        #print(f'logits_shape: {logits.shape}')                    \n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            #correct = AccuracyForMLM(logits, labels, attention_mask)           \n",
    "            #total_correct += correct.sum().item() \n",
    "            #total_len += attention_mask.sum().item()\n",
    "            #=========================================\n",
    "     \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                                   \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}'.format(epoch+1, epochs, itr, train_loss))\n",
    "                     \n",
    "                list_train_loss.append(train_loss)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                ####################################################################\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d6fbf-bc8a-49da-b2b3-732654e1fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e3f43-a44f-4d02-b1a3-289a4d7e727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bebde9-ddec-48b8-b822-19e40967a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론시 topk 알고리즘 사용\n",
    "def topk(probs, n=9):\n",
    "    probs = torch.softmax(probs, dim=-1)\n",
    "    \n",
    "    tokensProb, topIx = torch.topk(probs, k=n)\n",
    "    tokensProb = tokensProb / torch.sum(tokensProb)\n",
    "    \n",
    "    tokensProb = tokensProb.cpu().detach().numpy()\n",
    "    \n",
    "    choice = np.random.choice(n,1,p=tokensProb)\n",
    "    tokenId = topIx[choice][0]\n",
    "    \n",
    "    return int(tokenId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568582f6-de63-43f0-8310-6126b4e07165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize 추론 처리 함수 \n",
    "def model_infer(model, tokenizer, review, max_length=15):\n",
    "    review_encoded = tokenizer.encode(review)\n",
    "    result = review_encoded\n",
    "    initial_input = torch.tensor(review_encoded).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(initial_input)\n",
    "        \n",
    "        logits = outputs.logits[0,-1]\n",
    "        print(logits.shape)  # embedding 계수 출력됨\n",
    "        #result.append(topk(logits))\n",
    "        #print(result)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            input = torch.tensor(result).unsqueeze(0).to(device)\n",
    "            outputs = model(input)\n",
    "            logits = outputs.logits[0,-1]\n",
    "            res_id = topk(logits)\n",
    "            \n",
    "            if res_id == tokenizer.eos_token_id:\n",
    "                return tokenizer.decode(result)\n",
    "            else:\n",
    "                result.append(res_id)\n",
    "                \n",
    "    return tokenizer.decode(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e396e2d-a08b-4a83-b18f-b1a3b0c53d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A review is initially fed to the model.\n",
    "# A choice from the top-k choices is selected.\n",
    "# The choice is added to the summary and the current sequence is fed to the model.\n",
    "# Repeat steps 2 and 3 until either max_len is achieved or the EOS token is generated.\n",
    "\n",
    "context = '''\n",
    "전남도는 청정하고 안전한 신재생에너지 보급 확산을 위해 추진하는 ‘2022년 신재생에너지 보급 주택지원사업’ 참여 가구를 9일부터 6월 3일까지 모집한다고 밝혔다.\n",
    "신재생에너지 보급 주택지원사업은 주택에 태양광, 태양열, 연료전지, 지열 등 신재생에너지 설비를 설치하는 도민에게 정부 지원금 외에 도비와 시군비를 추가로 지원하는 사업이다.\n",
    "사업 대상자로 선정되면 설치비 자부담분(50%)의 40%를 도비와 시군비로 지원하기 때문에 비용부담을 줄일 수 있다.\n",
    "전남도는 올해 지방비 17억원을 들여 1600여 가구에 신재생에너지 설비를 보급할 계획이다.\n",
    "지원을 바라는 주택 소유자는 한국에너지공단 그린홈 누리집에서 회원가입 후 공단에 등록한 참여업체를 지정해 신청하면 된다.\n",
    "지방비 보조금은 한국에너지공단의 최종 사업 승인 후 해당 시군에 지원 신청을 하면 예산 범위에서 선착순 지원한다.\n",
    "자세한 사항은 한국에너지공단 광주전남지역본부, 전남도 에너지신산업과, 시군 에너지업무 담당 부서로 문의하면 된다.\n",
    "주택에 3KW 태양광 설비를 설치하면 총 설치비 516만원 중 국비 258만원과 추가로 지방비 103만원을 지원하기 때문에 155만원만 자부담하면 된다.\n",
    "3KW 태양광 설비를 설치한 가구는 월 4만5000원씩 연간 54만원의 전기요금을 절감할 수 있다.\n",
    "'''\n",
    "\n",
    "question = '전기요금 절감 금액은?'\n",
    "\n",
    "inputs = context + \"<question>\" + question + \"<answer>\"\n",
    "\n",
    "output = model_infer(model, tokenizer, inputs , 100)\n",
    "#print(output)\n",
    "\n",
    "answer = output.split(\"<answer>\")[1].strip()\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
