{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ad6ab2-0b69-4e3f-9e9f-e18a22057f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================\n",
    "# SKT Ko-GPT2 Text Generation 예제 \n",
    "# => https://github.com/SKT-AI/KoGPT2\n",
    "#====================================================================================================\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "model_path='../model/gpt-2/kogpt-2-ft-summarizer-0509/'\n",
    "#model_path='skt/kogpt2-base-v2'\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6161f3f8-4e74-4ec3-9c21-d912f7d0605c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bos_token = </s> 인 이유는 => 보통 훈련된 모델들은 </s>를 시작 과 종료 토큰으로 모두 사용한다.\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n",
    "\n",
    "tokenizer.tokenize(\"<s>안녕하세요. 한국어 GPT-2 입니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2aa374a-1dd2-48ad-8c67-424e7b5b8eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403456bc-c5a2-4ed0-ab9a-e595d9ce156a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125164032"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853a808b-4a51-46b3-a131-186e83bd5b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32016]])\n",
      "torch.Size([1, 56])\n",
      "tensor([32016,   739, 11294,  8363, 27073,  7514,  8263,  9355, 40612, 13168,\n",
      "         8022,  9167, 16210, 28597, 10771,  9447,  9429,  7756, 10199,  9737,\n",
      "         9327,  8705,  9249, 13312,  9368,  9430,  9623,  7426, 10956,  6958,\n",
      "        12503,  8102,  8267,  9025,  9341, 11200,  6824, 10089,  7252,  7182,\n",
      "         9724,   457,   459, 10473,  9837, 21049,   443,   405,  7657, 21334,\n",
      "        13993,  8367, 31872, 13940,   375,     1], device='cuda:0')\n",
      "날씨  봄철 미세먼지 때문에 호흡기 질환에 대한 관심이 높아지고 있는데 특히 황사 먼지가 심할 경우 호흡기를 통해 폐로 들어가기 쉬워질 수 있어 주의가 요구된다 <summarize>봄철에 공기청정기 필수\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '날씨'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "gen_ids = model.generate(input_ids.to(device),\n",
    "                         max_length=128,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "print(gen_ids.shape)\n",
    "print(gen_ids[0])\n",
    "\n",
    "# skip_special_tokens=True 로 해서 <s>, </s> 토큰들은 출력안 시킬수도 있음\n",
    "generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4774d539-6c60-4f7a-99fc-d547da123861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 tokenizer 파일로 저장\n",
    "#tokenizer.save_pretrained('kogpt2')\n",
    "#model.save_pretrained('kogpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b0c6a8-0b46-4d15-8496-5f972841a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation 테스트 해보는 함수 \n",
    "def eval_keywords(keywords):\n",
    "    model.eval()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        input_seq = \"<s>\" + keyword\n",
    "        generated = torch.tensor(tokenizer.encode(input_seq)).unsqueeze(0)\n",
    "        generated = generated.to(device)\n",
    "        sample_outputs = model.generate(generated,\n",
    "                                        do_sample = True,\n",
    "                                        top_k=30,\n",
    "                                        max_length=50,\n",
    "                                        top_p=0.90,\n",
    "                                        num_return_sequences=2)\n",
    "        \n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            # skip_special_tokens=True 로 해서 <s>, </s> 토큰들은 출력안함\n",
    "            print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            if i == 1:\n",
    "                print(\"\\n\")\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a74b7e-2703-48bf-9e79-ce6a967b3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 지미 카터 전 미국 대통령의 부인  카스트로 의원은 지난 16일 방송된 JTBC  슈퍼맨 김희재  미스터  에서는 자신의 친오빠인 고 김민경을 대신해 딸로서 딸내미를 키워주\n",
      "1: 지미 카터 전 미국 대통령이 지난달 15일 서울 신라호텔에서 열린 아시아 태평양 경제협력체(APEC) 정상회의에 참석해 한미동맹의 가치를 높이 평가하며 한국의 미래와 자유민주주의 체제의 미래에 대해 이야기하는 등 한반도 비핵화와 한반도\n",
      "\n",
      "\n",
      "0: 제임스 얼터너티브 록밴드 R&B의 리더 존 레스터가 18일 방송 예정인 미국 CBS 뉴스에 출연해 미국   2019 올해의 밴드  를 꼽으며 자신이 가장 사랑하는 밴드라고 밝히며  존 레\n",
      "1: 제임스 얼라이언스 CEO는 지난달 29일 삼성전자 서초사옥에서 삼성전자 관계자 및 임직원들이 모여  삼성전자서비스  삼성SDS와의 합병 추진에 대한 의견을 나누며  합병의 필요성에 대해 공감\n",
      "\n",
      "\n",
      "0: 수학전문기업 이투스교육 과 수학전문교육기업 에듀윌  입시전략연구소가  2019년 9월 21일부터 27일까지  2019 수능시험  EBS 교재  수험생 지원 이벤트 를 실시한다고 밝혔다 <\n",
      "1: 수학능력시험이 치러진 5일 수험생들은 시험장 앞에 줄을 선 채 고사장을 찾아 긴장한 모습이 역력했다 <summarize>수험생  늦잠 자려요     수\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 단어를 입력하여, text generation 해 봄\n",
    "keywords = [\"지미 카터\",\"제임스 얼\",\"수학\"]\n",
    "eval_keywords(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80b565-127a-495f-958a-63948658ed68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
