{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad58dd74-4231-4203-a18c-84bec7ab397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-ft_2022-05-11.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# kogpt2 를 이용한 text generation finetuning 훈련 예제\n",
    "# => https://github.com/mcelikkaya/medium_articles/blob/main/gtp2_training.ipynb\n",
    "# => https://github.com/gyunggyung/KoGPT2-FineTuning/\n",
    "#\n",
    "# => text generation 모델이므로  acc 구하는 것은 의미 없음(*따라서 train loss, val loss 만 구함)\n",
    "#\n",
    "# [훈련 dataset]\n",
    "# => input_ids = <s>text</s>\n",
    "# => labels = input_ids와 동일\n",
    "# => attiention_mask\n",
    "#\n",
    "# [text generation  훈련 과정]\n",
    "# \n",
    "# 1. gpt-2 모델 선언(GPT2LMHeadModel), tokenizer 선언(PreTrainedTokenizerFast)\n",
    "# 2. '<s>+문장+</s>' 식으로 된 훈련 dataset 생성\n",
    "# 3. 모델에 input_ids, lables, attention_mask 을 입력하여 훈련 시킴\n",
    "# \n",
    "# 원래 input_ids = 100,200,101,201,300,301 토큰이 입력된다면, labels은 input_ids 좌측으로 shift된 값\n",
    "# labels = 200,101,201,300,301 식으로 입력이 이루어 저야 하는데..\n",
    "# GPT2LMHeadModel 를 이용하면, labels = input_ids 와 똑같이 입력하면됨.\n",
    "# 이유는 GPT2LMHeadModel 내부적으로, 아래처럼 labels에 대해 shift 연산을 가지고 처리함\n",
    "#\n",
    "# [GPT2LMHeadModel 소스 일부]\n",
    "#if labels is not None:\n",
    "#        # Shift so that tokens < n predict n\n",
    "#        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#        shift_labels = labels[..., 1:].contiguous()\n",
    "#        # Flatten the tokens\n",
    "#        loss_fct = CrossEntropyLoss()\n",
    "#        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, TextGeneration_Dataset\n",
    "\n",
    "\n",
    "model_path='../model/gpt-2/kogpt-2/'\n",
    "#model_path='skt/kogpt2-base-v2'\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../model/gpt-2/kogpt-2-ft-0504/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-ft\", logfilename=\"../log/gpt2-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6579de-7702-4dc7-afbb-ce933a1640b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a2856-b2a5-41b0-99b7-dc70f79f2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318ddc5-a5ab-479f-94fc-c8963eeb69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation 테스트 해보는 함수 \n",
    "def eval_keywords(keywords):\n",
    "    model.eval()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        input_seq = \"<s>\" + keyword\n",
    "        generated = torch.tensor(tokenizer.encode(input_seq)).unsqueeze(0)\n",
    "        generated = generated.to(device)\n",
    "        sample_outputs = model.generate(generated,\n",
    "                                        do_sample = True,\n",
    "                                        top_k=30,\n",
    "                                        max_length=50,\n",
    "                                        top_p=0.90,\n",
    "                                        num_return_sequences=2)\n",
    "        \n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            if i == 1:\n",
    "                print(\"\\n\")\n",
    "                                            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528ec5e-63a4-4a8d-832a-e152f8f79e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어를 입력하여, text generation 해 봄\n",
    "keywords = [\"지미 카터\",\"제임스 얼\",\"수학\"]\n",
    "eval_keywords(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d74419-4abf-4fea-b826-ac92c7b1a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['주문은 가능하지만  수령은 2달 정도 걸릴 것 같네요', '이런  저희는 물건을 최대한 빠르게 받고 싶어요', '사무용품이나 가구 중 가장 먼저 바꿔야 할 것은 뭐라고 생각하나요', '아무래도 컴퓨터가 아닐까요  가장 많이 쓰니까요', '저는 모두가 같이 쓰는 회의실 책상을 먼저 바꿔야 한다고 생각해요']\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning 할 말뭉치를 불러옴 \n",
    "corpus_path = \"../korpora/mycorpus/2_대화체.txt\"\n",
    "all_sentences = []\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "            all_sentences.append(line.strip())  # strip() 계행문자 제거\n",
    "   \n",
    "print(all_sentences[10:15])\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e85dc2f-80ec-4385-9d8c-cd42edb72793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token_len:68\n"
     ]
    }
   ],
   "source": [
    "# 최대 토큰 계수를 구함.\n",
    "max_token_len = max([len(tokenizer.encode(s)) for s in tqdm(all_sentences)])\n",
    "print(f'max_token_len:{max_token_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1524cc-0941-4962-b11f-ed8a01d563dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277313c38e814ff48e20e3a4877cf678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([    1,  9046, 11031,  9826, 11686,   739, 14525,  8135,  9045,  7187,\n",
      "         9465, 47203,  9031,  9144,  7098,  8084,     1,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3]), tensor([    1, 10165,   739,  9265, 22633, 14579, 19226, 15292, 10884, 25799,\n",
      "         8084,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3]), tensor([    1, 11709, 21153,  9185, 16947,  9044,  9278, 10635, 27729,  7991,\n",
      "         9337,  9362, 46651,  9329,  9658, 11698,  8084,     1,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])], [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])])\n"
     ]
    }
   ],
   "source": [
    "# TextGenerattion 데이터셋 생성\n",
    "# => bos_token + 문장 + eos_token\n",
    "dataset = TextGeneration_Dataset(all_sentences, tokenizer, max_length=max_token_len)\n",
    "print(dataset[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cce83-760b-4f59-ac52-0a685505d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 test 데이터를 나눔\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'train_size:{len(train_set)}')\n",
    "print(f'val_size:{len(val_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469c184-da2f-44c5-85bc-860d1467dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 생성\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, sampler = RandomSampler(train_set), batch_size=batch_size)\n",
    "eval_loader = DataLoader(val_set, sampler = RandomSampler(val_set), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8fbbd-8814-4e89-8564-f4a4e321433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_data in eval_loader:\n",
    "    print(val_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5a33b-229e-4bd8-8b4f-08d7d3800ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "##################################################\n",
    "epochs = 2            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_test_len = 0\n",
    "total_correct = 0\n",
    "total_test_loss = 0\n",
    "total_test_loss_count = 0\n",
    "total_test_correct = 0\n",
    "\n",
    "list_train_loss = []\n",
    "list_validation_loss = []\n",
    "\n",
    "# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "model.zero_grad(set_to_none=True)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data[0].to(device)\n",
    "        attention_mask = data[1].to(device)\n",
    "        \n",
    "         # labels은 input_ids와 동일하게 입력 (*GPT2LMHeadModel 을 이용하는 경우, 내부적으로 labels 값에 대해 shift 연산처리를 해서 손실 구함)\n",
    "        labels = data[0].to(device)  \n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # logits_shape: torch.Size([32, 68, 51200])\n",
    "        # => batch_size, sequence_max_len, token_len\n",
    "        #print(f'logits_shape: {logits.shape}')                    \n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            #correct = AccuracyForMLM(logits, labels, attention_mask)           \n",
    "            #total_correct += correct.sum().item() \n",
    "            #total_len += attention_mask.sum().item()\n",
    "            #=========================================\n",
    "     \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                #train_acc = total_correct/total_len\n",
    "                \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "                for data in tqdm(eval_loader):\n",
    "                #for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data[0].to(device)\n",
    "                    attention_mask = data[1].to(device)\n",
    "                    labels = data[0].to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "                        \n",
    "                        total_test_loss += loss.item()\n",
    "                        total_test_loss_count += 1\n",
    "                        \n",
    "                        #===========================================\n",
    "                        # 정확도(Accurarcy) 계산\n",
    "                        #correct = AccuracyForMLM(logits, labels, attention_mask)           \n",
    "                        #total_test_correct += correct.sum().item() \n",
    "                        #total_test_len += attention_mask.sum().item()\n",
    "                        #=========================================\n",
    "                        \n",
    "                #val_acc = total_test_correct/total_test_len        \n",
    "                val_loss = total_test_loss/total_test_loss_count\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Val Loss: {}'.format(epoch+1, epochs, itr, train_loss, val_loss))\n",
    "                #logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {}, Val Acc:{}({}/{})'.format(epoch+1, epochs, itr, train_loss, train_acc, val_loss, val_acc, total_test_correct, total_test_len))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_validation_loss.append(val_loss)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_test_loss = 0\n",
    "                total_test_correct = 0\n",
    "                total_test_len = 0\n",
    "                total_correct = 0\n",
    "                total_test_loss = 0\n",
    "                total_test_loss_count = 0\n",
    "                ####################################################################\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290d0f0-489c-4374-b3a5-cf387c444b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c09d5a-6b91-4418-a320-e4602ad3a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "plt.plot(list_validation_loss, label='val Loss')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01bd04-761d-45db-96de-dd04b338880d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
