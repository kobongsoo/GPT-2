{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-ft_2022-05-04.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# kogpt2 를 이용한 summarize 예시\n",
    "# => \n",
    "# => https://www.nbshare.io/notebook/764386829/Amazon-Review-Summarization-Using-GPT-2-And-PyTorch/\n",
    "#\n",
    "# => text generation 모델이므로  acc 구하는 것은 의미 없음(*따라서 train loss, val loss 만 구함)\n",
    "#\n",
    "# [훈련 dataset]\n",
    "# => input_ids = <s>text</s>\n",
    "# => labels = input_ids와 동일\n",
    "# => attiention_mask\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "from summarizer import TransformerSummarizer\n",
    "model_path='../model/gpt-2/kogpt-2/'\n",
    "#model_path='skt/kogpt2-base-v2'\n",
    "#model_path = \"gpt2-medium\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../model/gpt-2/kogpt-2-ft-summarizer-0504/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-ft\", logfilename=\"../log/gpt2-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63aad3da-2f1b-4574-b907-290e54860a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약문 corpus 파일 열기 \n",
    "corpus_path = \"../korpora/mycorpus/newspaper.csv\"\n",
    "with open(corpus_path, \"r\") as reviews_raw:\n",
    "    reviews = reviews_raw.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076f092e-6cee-4c3c-b8c4-f09582aec92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['충주시는 민간보조사업의 증가와 보조금 집행관리에 대한 부당 행위가 증가함에따라 15일부터 25일까지 보조금 실태를 파악한 후 8월15일까지 세부감사를 진행  운영실태 전반에 대한 자체 감사를 실시할 계획이라고 밝혔다 ,충주시  민간지원 보조사업 대형축제 운영 감사 돌입\\n',\n",
       " '국무조정실은 8일 오후 대전시청에서  대전지역 규제혁신 현장간담회 를 열고 대전과 충남에 취약한 뿌리산업 육성방안으로 규제개선을 논의하였으며  관계자는 기업과 국민들이 체감할 수 있는 규제개선이 이루어지도록 최대한 노력할 것이라고 밝혔다 ,대전도 뿌리산업 특화단지 길 열린다   국무조정실 규제개선키로\\n',\n",
       " '중국 경제일간지 21세기경제보도는 중국 대형 생명보험사인 차이나라이프가  차이나라이프 중흥 이라는 전략적 목표를 세우고 비즈니스 중심의 조직 시스템 구축과 인터넷 생명보험회사 설립에 착수 중이라고 17일 보도하였다 ,중국 생보사 차이나라이프  인터넷 보험사 설립 추진\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707e85e0-56cb-4126-aa7e-8c0467d62eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3091\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72731ac2-985c-4d3a-bb5c-36e38ac7c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews 문장은 text, 요약문 식으로 되어 있다.\n",
    "# => 이 구분자 , 를 <summarize> 로 변경함.\n",
    "reviews = [review.replace(\",\", \"<summarize>\") for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae098913-5845-419e-b823-4e69904222cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국무조정실은 8일 오후 대전시청에서  대전지역 규제혁신 현장간담회 를 열고 대전과 충남에 취약한 뿌리산업 육성방안으로 규제개선을 논의하였으며  관계자는 기업과 국민들이 체감할 수 있는 규제개선이 이루어지도록 최대한 노력할 것이라고 밝혔다 <summarize>대전도 뿌리산업 특화단지 길 열린다   국무조정실 규제개선키로\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118c7702-7c8a-4e5b-8074-36c07997911b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.62924619864122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 단어의 길이를 얻어옴\n",
    "avg_length = sum([len(review.split()) for review in reviews])/len(reviews)\n",
    "avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e17114-82ab-4e8e-aa7c-c0a65981e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 단어의 길이보다 길게 설정\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0154b5d0-18f0-4bea-82f0-6c2415c4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로딩 \n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06240c79-1531-4eab-a43f-5c608cbaabbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51fbc0f-ccc4-4891-aac2-0b08aebfc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9724, 457, 459, 10473, 9837, 21049, 443, 405]\n",
      "***<summarize> token len:8\n"
     ]
    }
   ],
   "source": [
    "# 구분자 <summarize> 토큰의 길이를 얻어옴\n",
    "print(tokenizer.encode(\"<summarize>\"))\n",
    "extra_length = len(tokenizer.encode(\"<summarize>\"))\n",
    "print(f'***<summarize> token len:{extra_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6015caf8-b943-4897-ab70-01888598eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 설정 함수\n",
    "class summarizeDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, max_len, summarize_token_len):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.sentences = sentences\n",
    "        self.summarize_token_len = summarize_token_len\n",
    "        self.result = []\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            # 한 문장 뒤에 </s>(EOS 토큰) 추가\n",
    "            tokenized = self.tokenizer.encode(sentence + self.eos)\n",
    "            #print(tokenized)\n",
    "            # padd \n",
    "            padded = self.pad_truncate(tokenized)\n",
    "            \n",
    "            # 출력\n",
    "            self.result.append(torch.tensor(padded))\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.result[item]\n",
    "    \n",
    "    # padd 붙이는 함수\n",
    "    def pad_truncate(self, name):\n",
    "        \n",
    "        name_length = len(name) - self.summarize_token_len\n",
    "        \n",
    "        if name_length < self.max_len:\n",
    "            difference = self.max_len - name_length\n",
    "            result = name + [self.eos_id] * difference\n",
    "        elif name_length > self.max_len:\n",
    "            result = name[:self.max_len + 3]+[self.eos_id]\n",
    "        else:\n",
    "            result = name\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9086b2c1-8580-4609-b34b-9c4c857a4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 만듬\n",
    "dataset = summarizeDataset(tokenizer, reviews, max_length, extra_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa1e5cf9-562c-43bd-9db9-8d1e387a7406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14932, 13305, 16428, 16308, 21734, 13899,  7888, 29718,   739, 13899,\n",
       "         9635, 17143, 25002, 16380,  6826,  7191,  8765, 36055, 20759,  9026,\n",
       "        12391, 27775,  8022,  9499, 16455, 13961, 10258, 16891,  7607, 21609,\n",
       "        17143,  6841, 10402, 13564, 15423,   739,  9804,  9599, 50865,  9888,\n",
       "         9136, 48262,  8705,  9025,  9080, 17143,  6841, 10886,  9442, 25682,\n",
       "        19226, 10805,  8705, 11839, 28296,  7182,  9724,   457,   459, 10473,\n",
       "         9837, 21049,   443,   405, 10238,  7235, 13961, 10258,  9125,  8756,\n",
       "        18081,  9367, 13363,  7182,   739,   739, 14932, 13305,  7892, 17143,\n",
       "        32931,  8511,  7426,   375,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499222e0-0c3b-4102-ab3c-252d652560f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 생성 \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2286a2-b29b-4c47-bf90-26ebf05c8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\", transformer_model_key=model_path)\n",
    "#full = ''.join(GPT2_model(body, min_length=60))\n",
    "#print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498a10a-3e25-4336-8686-b23403cc7ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b9a64-2c97-450b-9dad-4581f90784c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371b06c-48c5-4e96-855d-07b31838755d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
