{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad58dd74-4231-4203-a18c-84bec7ab397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../log/bwdataset_2022-05-04.log\n",
      "logfilepath:../log/qnadataset_2022-05-04.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/gpt2-ft_2022-05-04.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# kogpt2 를 이용한 text generation finetuning 훈련 예제\n",
    "# => https://github.com/mcelikkaya/medium_articles/blob/main/gtp2_training.ipynb\n",
    "# => https://github.com/gyunggyung/KoGPT2-FineTuning/\n",
    "#\n",
    "# => text generation 모델이므로  acc 구하는 것은 의미 없음(*따라서 train loss, val loss 만 구함)\n",
    "#\n",
    "# [훈련 dataset]\n",
    "# => input_ids = <s>text</s>\n",
    "# => labels = input_ids와 동일\n",
    "# => attiention_mask\n",
    "#\n",
    "# [text generation  훈련 과정]\n",
    "# \n",
    "# 1. gpt-2 모델 선언(GPT2LMHeadModel), tokenizer 선언(PreTrainedTokenizerFast)\n",
    "# 2. '<s>+문장+</s>' 식으로 된 훈련 dataset 생성\n",
    "# 3. 모델에 input_ids, lables, attention_mask 을 입력하여 훈련 시킴\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "\n",
    "\n",
    "model_path='../model/gpt-2/kogpt2/'\n",
    "#model_path='skt/kogpt2-base-v2'\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../model/gpt-2/kogpt-2-ft-0504/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"gpt2-ft\", logfilename=\"../log/gpt2-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6579de-7702-4dc7-afbb-ce933a1640b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path,\n",
    "                                                   bos_token='</s>',\n",
    "                                                   eos_token='</s>',\n",
    "                                                   unk_token='<unk>',\n",
    "                                                   pad_token='<pad>',\n",
    "                                                   mask_token='<mask>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300a2856-b2a5-41b0-99b7-dc70f79f2845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e318ddc5-a5ab-479f-94fc-c8963eeb69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation 테스트 해보는 함수 \n",
    "def eval_keywords(keywords):\n",
    "    model.eval()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        input_seq = \"<s>\" + keyword\n",
    "        generated = torch.tensor(tokenizer.encode(input_seq)).unsqueeze(0)\n",
    "        generated = generated.to(device)\n",
    "        sample_outputs = model.generate(generated,\n",
    "                                        do_sample = True,\n",
    "                                        top_k=30,\n",
    "                                        max_length=50,\n",
    "                                        top_p=0.90,\n",
    "                                        num_return_sequences=2)\n",
    "        \n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            if i == 1:\n",
    "                print(\"\\n\")\n",
    "                                            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a528ec5e-63a4-4a8d-832a-e152f8f79e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 지미 카터 전 대통령도 '북한의 도발에는 단호하게 대응한다'는 입장을 밝힌 바 있다.\n",
      "그러나 북한은 도발시 단호한 대응이 가능하다는 점을 강조하고 있다. 새누리당은 12일 문재인 정부의 대북정책\n",
      "1: 지미 카터 전 대통령의 자서전 ‘나의 인생’에 나오는 한 명인 카터 전 대통령은 이라크 전쟁에 대한 그의 신념이 가장 강한 인물입니다.\n",
      "그의 신념은 이라크에 대한 그의 군사적 개입을 거부하면서도 “미국의 전쟁 수행은 결코 용납\n",
      "\n",
      "\n",
      "0: 제임스 얼레이 ( James Ellay ) : 미국의 유명 축구 선수\n",
      "1996년 3월 23일 미국 노스캐롤라이나주 윌밍턴에서 열린 FA컵 준결승에서 데뷔골을 넣었으며, 이후 2005년 5월 19일 필라델피아 필리스와의\n",
      "1: 제임스 얼티밋은 \"한국은 '다문화' 국가로서 한국인을 어떻게 보호할지 걱정스럽다\"고 밝혔다.\n",
      "이어 그는 \"한국인과 결혼한 중국인들은 결혼할 때 결혼기념일을 축하할 때 한국어로 노래를 부를 때 한국어로 노래\n",
      "\n",
      "\n",
      "0: 수학. 과학실험을 통해 자신의 수학 실력을 객관적으로 점검하는 방법을 소개한다.\n",
      "아름다운 자연 풍광을 자랑하는 강원도 평창군 봉평면에 위치한 한국천문연구원은 올해 2월과 9월 두 차례에 걸쳐 천체망원경으로 측정한 별을 찾아냈다.\n",
      "2월\n",
      "1: 수학, 수학 등 기초과학이 핵심과학과 연계된다면 그 응용력은 배가된다.\n",
      "그런데 이 분야는 기초과학이 아닌 학문인 것이다.\n",
      "이런 기초과학이 우리 사회와 사회 모든 영역에서 차지하는 비중을 감안한다면 과학기술에 대한 일반 국민의\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 단어를 입력하여, text generation 해 봄\n",
    "keywords = [\"지미 카터\",\"제임스 얼\",\"수학\"]\n",
    "eval_keywords(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d74419-4abf-4fea-b826-ac92c7b1a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning 할 말뭉치를 불러옴 \n",
    "corpus_path = \"../korpora/mycorpus/2_대화체.txt\"\n",
    "all_sentences = []\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "            all_sentences.append(line.strip())  # strip() 계행문자 제거\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb527de-3339-4267-baa8-bca39d4b5197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['주문은 가능하지만  수령은 2달 정도 걸릴 것 같네요', '이런  저희는 물건을 최대한 빠르게 받고 싶어요', '사무용품이나 가구 중 가장 먼저 바꿔야 할 것은 뭐라고 생각하나요', '아무래도 컴퓨터가 아닐까요  가장 많이 쓰니까요', '저는 모두가 같이 쓰는 회의실 책상을 먼저 바꿔야 한다고 생각해요']\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(all_sentences[10:15])\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e85dc2f-80ec-4385-9d8c-cd42edb72793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token_len:68\n"
     ]
    }
   ],
   "source": [
    "# 최대 토큰 계수를 구함.\n",
    "max_token_len = max([len(tokenizer.encode(s)) for s in all_sentences])\n",
    "print(f'max_token_len:{max_token_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4ed7d0-0616-4cc6-ae73-a91c7028a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mytokenier 설정 \n",
    "def tokenizer_seq(sentence, tokenizer, max_length):\n",
    "    return tokenizer('<s>' + sentence + '</s>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "class myDataSet(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, gpt2_type=\"gpt2\", max_length=128):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            encodings = tokenizer_seq(sentence, tokenizer, max_length)\n",
    "            #print(encodings) break\n",
    "            self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
    "            self.attention_masks.append(torch.tensor(encodings['attention_mask']))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6bcf3c-de10-44c4-a3e1-9d705d78c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = myDataSet(all_sentences, tokenizer, max_length=max_token_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e49870ec-7c99-41e6-90b7-1bb7e84dc767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([    0,  9046, 11031,  9826, 11686,   739, 14525,  8135,  9045,  7187,\n",
      "         9465, 47203,  9031,  9144,  7098,  8084,     1,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3]), tensor([    0, 10165,   739,  9265, 22633, 14579, 19226, 15292, 10884, 25799,\n",
      "         8084,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3]), tensor([    0, 11709, 21153,  9185, 16947,  9044,  9278, 10635, 27729,  7991,\n",
      "         9337,  9362, 46651,  9329,  9658, 11698,  8084,     1,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3])], [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4cce83-760b-4f59-ac52-0a685505d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:90000\n",
      "val_size:10000\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 test 데이터를 나눔\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'train_size:{len(train_set)}')\n",
    "print(f'val_size:{len(val_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c469c184-da2f-44c5-85bc-860d1467dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 생성\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, sampler = RandomSampler(train_set), batch_size=batch_size)\n",
    "eval_loader = DataLoader(val_set, sampler = RandomSampler(val_set), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d8fbbd-8814-4e89-8564-f4a4e321433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[    0, 42108, 22500,  ...,     3,     3,     3],\n",
      "        [    0, 10223, 11190,  ...,     3,     3,     3],\n",
      "        [    0,  9183, 15344,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    0, 10769,  8146,  ...,     3,     3,     3],\n",
      "        [    0,  9873,   739,  ...,     3,     3,     3],\n",
      "        [    0, 12478,  9148,  ...,     3,     3,     3]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "for val_data in eval_loader:\n",
    "    print(val_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d5a33b-229e-4bd8-8b4f-08d7d3800ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f162941e88e4294a413699e80b0baf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fcaf54e754422db33e8dc4520c8a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea7e2fd7a24b3baaffb55b4818dc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:28:54,831 - gpt2-ft - INFO - [Epoch 1/2] Iteration 281 -> Train Loss: 2.3892, Train Acc: 0.0116, Val Loss: 0.8258537600596492, Val Acc:0.0055193053034391405(900/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d6ccbb899d4fe8aa7e92e2dbbfec1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:29:33,023 - gpt2-ft - INFO - [Epoch 1/2] Iteration 562 -> Train Loss: 0.8170, Train Acc: 0.0013, Val Loss: 0.8209881314073508, Val Acc:0.0004660746700681941(76/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609d7a646c3d4cff9510e10c08180b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:30:11,243 - gpt2-ft - INFO - [Epoch 1/2] Iteration 843 -> Train Loss: 0.8108, Train Acc: 0.0004, Val Loss: 0.8100547428710011, Val Acc:0.002213854682823922(361/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5d9e9c6ef941ecb2b4d6dedd1b4373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:30:49,843 - gpt2-ft - INFO - [Epoch 1/2] Iteration 1124 -> Train Loss: 0.8011, Train Acc: 0.0002, Val Loss: 0.8006997885414586, Val Acc:0.0005396654074473826(88/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e219608442394770848c981eacfd1a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:31:28,133 - gpt2-ft - INFO - [Epoch 1/2] Iteration 1405 -> Train Loss: 0.7951, Train Acc: 0.0001, Val Loss: 0.7932503143438516, Val Acc:0.0012571750968944709(205/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4e7a19d5d045e8bde3a92abb34682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:32:06,674 - gpt2-ft - INFO - [Epoch 1/2] Iteration 1686 -> Train Loss: 0.7907, Train Acc: 0.0001, Val Loss: 0.78872730175908, Val Acc:0.00043541186282686553(71/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07389b36b17a4c06abf77a1ec7cd4677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:32:44,773 - gpt2-ft - INFO - [Epoch 1/2] Iteration 1967 -> Train Loss: 0.7815, Train Acc: 0.0001, Val Loss: 0.783137516282237, Val Acc:0.00042927930137859983(70/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd58d77a7fb4c4882fd894e688e793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:33:22,952 - gpt2-ft - INFO - [Epoch 1/2] Iteration 2248 -> Train Loss: 0.7814, Train Acc: 0.0001, Val Loss: 0.7769473608309468, Val Acc:0.0005090026002060541(83/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8d926fec8a4bf08a833906a476fe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:34:01,169 - gpt2-ft - INFO - [Epoch 1/2] Iteration 2529 -> Train Loss: 0.7729, Train Acc: 0.0001, Val Loss: 0.7742940492142504, Val Acc:0.0005580630917921798(91/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6837f28ff02f433191c2250cc0039a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:34:39,479 - gpt2-ft - INFO - [Epoch 1/2] Iteration 2810 -> Train Loss: 0.7723, Train Acc: 0.0001, Val Loss: 0.7695069366369766, Val Acc:0.0004844723544129912(79/163064)\n",
      "2022-05-04 13:34:40,703 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/kogpt-2-ft-0504/batch:32-ep:2-lr:0.000030000-5m4d-13:34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1226417e87bc45f396d313c5f92596be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ac32ecdcae424a8584eb462c84664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:35:19,066 - gpt2-ft - INFO - [Epoch 2/2] Iteration 3091 -> Train Loss: 0.7194, Train Acc: 0.0001, Val Loss: 0.7703166476453835, Val Acc:0.00025143501937889415(41/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88a198ec3f84d37aeeb1d3ee52c1060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:35:57,742 - gpt2-ft - INFO - [Epoch 2/2] Iteration 3372 -> Train Loss: 0.6884, Train Acc: 0.0000, Val Loss: 0.7713381331949569, Val Acc:0.0003004955109650199(49/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc62add128c54449ba7176016c77daee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:36:36,461 - gpt2-ft - INFO - [Epoch 2/2] Iteration 3653 -> Train Loss: 0.6896, Train Acc: 0.0000, Val Loss: 0.7698388701429764, Val Acc:0.000275965265171957(45/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f570795a984d27bd9ee8651eebae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:37:14,974 - gpt2-ft - INFO - [Epoch 2/2] Iteration 3934 -> Train Loss: 0.6878, Train Acc: 0.0000, Val Loss: 0.7667432711147272, Val Acc:0.0001471814747583771(24/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7d6cb962e84201b1fe2e33c5ea8862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:37:53,445 - gpt2-ft - INFO - [Epoch 2/2] Iteration 4215 -> Train Loss: 0.6849, Train Acc: 0.0000, Val Loss: 0.7647632962217727, Val Acc:0.0003004955109650199(49/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35a52341e004c5c81a1b151eb711d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:38:31,836 - gpt2-ft - INFO - [Epoch 2/2] Iteration 4496 -> Train Loss: 0.6840, Train Acc: 0.0000, Val Loss: 0.7628507145677512, Val Acc:0.0002085070892410342(34/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c93998fafe466c98e19fd73a4f8a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:39:10,199 - gpt2-ft - INFO - [Epoch 2/2] Iteration 4777 -> Train Loss: 0.6850, Train Acc: 0.0000, Val Loss: 0.7612530834758624, Val Acc:0.00022077221213756562(36/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4b6a2e1e4c4fdb85b300645a5651cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:39:48,664 - gpt2-ft - INFO - [Epoch 2/2] Iteration 5058 -> Train Loss: 0.6789, Train Acc: 0.0000, Val Loss: 0.7601060968237563, Val Acc:0.00022690477358583132(37/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4198858c140340fdb11201c2a60c0e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:40:27,045 - gpt2-ft - INFO - [Epoch 2/2] Iteration 5339 -> Train Loss: 0.6807, Train Acc: 0.0000, Val Loss: 0.7588890490059654, Val Acc:0.00033115831820634844(54/163064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a849cd6344044bb182aacbc06f0ccf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 13:41:05,529 - gpt2-ft - INFO - [Epoch 2/2] Iteration 5620 -> Train Loss: 0.6810, Train Acc: 0.0000, Val Loss: 0.7578518710578211, Val Acc:0.00024530245793062845(40/163064)\n",
      "2022-05-04 13:41:07,042 - bwpdataset - INFO - ==> save_model : ../model/gpt-2/kogpt-2-ft-0504/batch:32-ep:2-lr:0.000030000-5m4d-13:41\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "##################################################\n",
    "epochs = 2            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_test_len = 0\n",
    "total_correct = 0\n",
    "total_test_loss = 0\n",
    "total_test_loss_count = 0\n",
    "total_test_correct = 0\n",
    "\n",
    "list_train_loss = []\n",
    "list_validation_loss = []\n",
    "\n",
    "# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "model.zero_grad(set_to_none=True)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data[0].to(device)\n",
    "        attention_mask = data[1].to(device)\n",
    "        labels = data[0].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # logits_shape: torch.Size([32, 68, 51200])\n",
    "        # => batch_size, sequence_max_len, token_len\n",
    "        #print(f'logits_shape: {logits.shape}')                    \n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            #correct = AccuracyForMLM(logits, labels, attention_mask)           \n",
    "            #total_correct += correct.sum().item() \n",
    "            #total_len += attention_mask.sum().item()\n",
    "            #=========================================\n",
    "     \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                #train_acc = total_correct/total_len\n",
    "                \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "                for data in tqdm(eval_loader):\n",
    "                #for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data[0].to(device)\n",
    "                    attention_mask = data[1].to(device)\n",
    "                    labels = data[0].to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "                        \n",
    "                        total_test_loss += loss.item()\n",
    "                        total_test_loss_count += 1\n",
    "                        \n",
    "                        #===========================================\n",
    "                        # 정확도(Accurarcy) 계산\n",
    "                        #correct = AccuracyForMLM(logits, labels, attention_mask)           \n",
    "                        #total_test_correct += correct.sum().item() \n",
    "                        #total_test_len += attention_mask.sum().item()\n",
    "                        #=========================================\n",
    "                        \n",
    "                #val_acc = total_test_correct/total_test_len        \n",
    "                val_loss = total_test_loss/total_test_loss_count\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Val Loss: {}'.format(epoch+1, epochs, itr, train_loss, val_loss))\n",
    "                #logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {}, Val Acc:{}({}/{})'.format(epoch+1, epochs, itr, train_loss, train_acc, val_loss, val_acc, total_test_correct, total_test_len))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_validation_loss.append(val_loss)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_test_loss = 0\n",
    "                total_test_correct = 0\n",
    "                total_test_len = 0\n",
    "                total_correct = 0\n",
    "                total_test_loss = 0\n",
    "                total_test_loss_count = 0\n",
    "                ####################################################################\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290d0f0-489c-4374-b3a5-cf387c444b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c09d5a-6b91-4418-a320-e4602ad3a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "plt.plot(list_validation_loss, label='val Loss')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01bd04-761d-45db-96de-dd04b338880d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
