{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93590776-649f-495e-a82a-9e2a502e5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 신규 SetnecePieceBPETokenzer tokenizer vocab 생성 예제\n",
    "# => tokenizer 방식중, SetnecePieceBPETokenzer 를 이용하여 새롭게 tokenize vocab 을 생성하는 예제임\n",
    "# => OpenAI GPT-2에서는 ByteLevelBPETokenizer , kogpt-2에서는 SetnecePieceBPETokenzer 를 사용함.\n",
    "#\n",
    "# 참고자료 \n",
    "# https://gist.github.com/lovit/e11c57877aae4286ade4c203d6c26a32\n",
    "# https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477/2\n",
    "#\n",
    "#================================================================================\n",
    "# [과정] => Kogpt-2 tokenizer 방식\n",
    "#\n",
    "# 1. SentencePieceBPETokenzer 정의 후 훈련 \n",
    "#     => 훈련시, corpora 목록, vocab_size, min_frequency 등을 설정함\n",
    "#\n",
    "# 2. 훈련한 SentencePieceBPETokenzer 를 PreTrainedTokenizerFast 와 연동\n",
    "#     =>PreTrainedTokenizerFast(tokenizer_object=stokenizer)\n",
    "#\n",
    "# 3. PreTrainedTokenizerFast tokenizer 저장\n",
    "#     => transforer_tokenizer.save_pretrained(OUT_PATH) \n",
    "#     => 정상적으로 저장되면, 해당 폴더에 3개 json 파일이 생성됨(tokenizer.json, special_tokens_map.json, tokenizer_config.json)\n",
    "#================================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "# 말뭉치 지정\n",
    "corpus_path = '../../korpora/kowiki_20190620/wiki_20190620_small.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d335ed11-9ea4-4fc4-beda-bb1a44d90946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. SetnecePieceBPETokenzer 정의 후 훈련 \n",
    "stokenizer = SentencePieceBPETokenizer(add_prefix_space=True)\n",
    "\n",
    "# 훈련\n",
    "stokenizer.train(\n",
    "    files = [corpus_path],\n",
    "    vocab_size = 52000,  # 최대 vocab 계수 \n",
    "    special_tokens = [\"<cls>\", \"<eos>\", \"<mask>\", \"<unk>\", \"<pad>\"],  # speical token 지정\n",
    "    min_frequency = 5,   # 빈도수 \n",
    "    show_progress = True,\n",
    "    #limit_alphabet=10000, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94763339-0663-47af-9cf9-6cc22d703bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = stokenizer.get_vocab()\n",
    "print(f'vcoab 길이:{len(vocab)}')\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))  # sort 해서 vocab 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf959e0-e83e-4766-aac3-b285d349b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>idx   : [109, 75, 8968, 81]\n",
      "=>tokens: ['▁', 'h', 'ell', 'o']\n",
      "=>offset: [(0, 1), (0, 1), (1, 4), (4, 5)]\n",
      "=>decode: hello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "sentence = 'hello'\n",
    "output = stokenizer.encode(sentence)\n",
    "print('=>idx   : %s'%output.ids)\n",
    "print('=>tokens: %s'%output.tokens)\n",
    "print('=>offset: %s'%output.offsets)\n",
    "print('=>decode: %s\\n'%stokenizer.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b169f4-49f6-459b-b170-ea4df1d4573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 훈련한 SetnecePieceBPETokenzer 를 PreTrainedTokenizerFast 와 연동\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "transforer_tokenizer = PreTrainedTokenizerFast(tokenizer_object=stokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a5a6a0-bf1c-49b5-85e4-0f3ccec371c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9571, 2448, 1050, 6670, 5255, 647, 1554]\n",
      "나는 오늘 아침을 먹었다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "sentence = '나는 오늘 아침밥을 먹었다.'\n",
    "output = transforer_tokenizer.encode(sentence)\n",
    "print(output)\n",
    "decode = transforer_tokenizer.decode(output)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec6b670-a9c8-4ef2-81c5-0eab5e7f1e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mytoken/tokenizer_config.json',\n",
       " './mytoken/special_tokens_map.json',\n",
       " './mytoken/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PreTrainedTokenizerFast tokenizer 저장\n",
    "import os\n",
    "OUT_PATH = './mytoken'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "transforer_tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee87c75c-cacb-413e-b122-97cf3945516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file gpt-2/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "file gpt-2/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167, 224, 246, 167, 232, 242, 23821, 246, 97, 167, 232, 246, 23821, 243, 226, 168, 117, 101, 167, 108, 98, 35975, 226, 31619, 101, 117, 168, 245, 230, 46695, 97, 13]\n",
      "나는 오늘 아침밥을 먹었다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "#OUT_PATH = 'gpt-2'\n",
    "# 저장된 tokenzier 불러와서 확인\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(OUT_PATH,\n",
    "                                            bos_token='<cls>',\n",
    "                                            eos_token='<eos>',\n",
    "                                            unk_token='<unk>',\n",
    "                                            pad_token='<pad>',\n",
    "                                            mask_token='<mask>')\n",
    "\n",
    "# 테스트\n",
    "sentence = '나는 오늘 아침밥을 먹었다.'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(output)\n",
    "decode = tokenizer.decode(output)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ace0d6-750c-41c5-99b1-62d88721b784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 오�'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([23821, 246, 97, 167])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465e1d0-546e-4d32-bcb9-f2f51cdd6058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
