{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93590776-649f-495e-a82a-9e2a502e5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 신규 ByteLevelBPETokenizer tokenizer vocab 생성 예제\n",
    "# => tokenizer 방식중, ByteLevelBPETokenizer 를 이용하여 새롭게 tokenize vocab 을 생성하는 예제임(GPT에서는 SetnecePieceBPETokenzer 를 이용함)\n",
    "# => 아래 과정은은 OpenAI GPT-2, ByteLevelBPETokenizer 를 사용함\n",
    "# => 참고로 kogpt-2 방식으로, SetnecePieceBPETokenzer 를 사용함.\n",
    "# \n",
    "# 참고자료 \n",
    "# https://gist.github.com/lovit/e11c57877aae4286ade4c203d6c26a32\n",
    "# https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477/2\n",
    "#\n",
    "#================================================================================\n",
    "# [과정] => gpt-2 tokenizer 방식\n",
    "#\n",
    "# 1. ByteLevelBPETokenizer 정의 후 훈련 \n",
    "#     => 훈련시, corpora 목록, vocab_size, min_frequency 등을 설정함\n",
    "#\n",
    "# 2. 훈련한 ByteLevelBPETokenizer 저장\n",
    "#     => stokenizer.save_model(OUT_PATH)\n",
    "#     => 이대 저장되면, 해당 폴더에 2개의 파일이 생성됨(merges.txt, vocab.json)\n",
    "#\n",
    "# 3. GPT2TokenizerFast로 merges.txt, vocab.json 파일 불러옴\n",
    "#     => transforer_tokenizer = GPT2TokenizerFast(vocab_file=vocab_path, merges_file=merges_path, add_prefix_space = True)\n",
    "#\n",
    "# 4. GPT2TokenizerFast tokenizer 저장\n",
    "#     => transforer_tokenizer.save_pretrained(OUT_PATH) \n",
    "#     => 정상적으로 저장되면, 해당 폴더에 3개 json 파일이 생성됨(tokenizer.json, special_tokens_map.json, tokenizer_config.json)\n",
    "#================================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "# 말뭉치 지정\n",
    "corpus_path = '../../korpora/kowiki_20190620/wiki_test.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f5972c-a909-4de9-a066-cb1eb402bfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "vcoab 길이:710\n"
     ]
    }
   ],
   "source": [
    "stokenizer = ByteLevelBPETokenizer(add_prefix_space = False, lowercase = False)\n",
    "\n",
    "# 훈련\n",
    "stokenizer.train(\n",
    "    files = [corpus_path],\n",
    "    vocab_size = 52000,  # 최대 vocab 계수 \n",
    "    special_tokens = [\"<cls>\", \"<eos>\", \"<mask>\", \"<unk>\", \"<pad>\"],  # speical token 지정\n",
    "    min_frequency = 5,   # 빈도수 \n",
    "    show_progress = True,\n",
    "    #limit_alphabet=10000, \n",
    ")\n",
    "\n",
    "vocab = stokenizer.get_vocab()\n",
    "print(f'vcoab 길이:{len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3398926-138e-4d64-bfd0-f25563e46343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>idx   : [172, 306, 287, 340, 375, 237, 251, 405, 311, 432, 113, 103, 292, 263, 106, 122, 362, 18]\n",
      "=>tokens: ['ë', 'Ĥĺ', 'ëĬĶ', 'Ġìĺ', '¤ë', 'Ĭ', 'ĺ', 'ĠìķĦ', 'ì¹', '¨ë', '°', '¥', 'ìĿĦ', 'Ġë', '¨', '¹', 'ìĹĪëĭ¤', '.']\n",
      "=>offset: [(0, 1), (0, 1), (1, 2), (2, 4), (3, 5), (4, 5), (4, 5), (5, 7), (7, 8), (7, 9), (8, 9), (8, 9), (9, 10), (10, 12), (11, 12), (11, 12), (12, 14), (14, 15)]\n",
      "=>decode: 나는 오늘 아침밥을 먹었다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "sentence = '나는 오늘 아침밥을 먹었다.'\n",
    "output = stokenizer.encode(sentence)\n",
    "print('=>idx   : %s'%output.ids)\n",
    "print('=>tokens: %s'%output.tokens)\n",
    "print('=>offset: %s'%output.offsets)\n",
    "print('=>decode: %s\\n'%stokenizer.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a868f40f-f5f8-4bc6-985b-6896ef0afb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./mytokenbpe/vocab.json', './mytokenbpe/merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "OUT_PATH = './mytokenbpe'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "stokenizer.save_model(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b169f4-49f6-459b-b170-ea4df1d4573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GPT2TokenizerFast로 merges.txt, vocab.json 파일 불러옴\n",
    "from transformers import GPT2TokenizerFast\n",
    "vocab_path = './mytokenbpe/vocab.json'\n",
    "merges_path = './mytokenbpe/merges.txt'\n",
    "\n",
    "tokenizer = GPT2TokenizerFast(vocab_file=vocab_path, merges_file=merges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a5a6a0-bf1c-49b5-85e4-0f3ccec371c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172, 306, 287, 340, 375, 237, 251, 405, 311, 432, 113, 103, 292, 263, 106, 122, 362, 18]\n",
      "나는 오늘 아침밥을 먹었다.\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "sentence = '나는 오늘 아침밥을 먹었다.'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(output)\n",
    "decode = tokenizer.decode(output)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec6b670-a9c8-4ef2-81c5-0eab5e7f1e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mytoken_BPE/tokenizer_config.json',\n",
       " './mytoken_BPE/special_tokens_map.json',\n",
       " './mytoken_BPE/vocab.json',\n",
       " './mytoken_BPE/merges.txt',\n",
       " './mytoken_BPE/added_tokens.json',\n",
       " './mytoken_BPE/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PreTrainedTokenizerFast tokenizer 저장\n",
    "import os\n",
    "OUT_PATH = './mytoken_BPE'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee87c75c-cacb-413e-b122-97cf3945516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172, 306, 287, 340, 375, 237, 251, 405, 311, 432, 113, 103, 292, 263, 106, 122, 362, 18]\n",
      "나는 오늘 아침밥을 먹었다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "#OUT_PATH = 'gpt-2'\n",
    "# 저장된 tokenzier 불러와서 확인\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(OUT_PATH,\n",
    "                                            bos_token='<cls>',\n",
    "                                            eos_token='<eos>',\n",
    "                                            unk_token='<unk>',\n",
    "                                            pad_token='<pad>',\n",
    "                                            mask_token='<mask>')\n",
    "\n",
    "# 테스트\n",
    "sentence = '나는 오늘 아침밥을 먹었다.'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(output)\n",
    "decode = tokenizer.decode(output)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ace0d6-750c-41c5-99b1-62d88721b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([23821, 246, 97, 167])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465e1d0-546e-4d32-bcb9-f2f51cdd6058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
